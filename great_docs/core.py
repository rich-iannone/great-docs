import os
import re
import shutil
from importlib import resources
from pathlib import Path

import yaml

from .config import Config


class GreatDocs:
    """
    GreatDocs class for creating beautiful API documentation sites.

    This class provides methods to install assets and configure
    Quarto projects with the great-docs styling and functionality.
    """

    def __init__(self, project_path: str | None = None):
        """
        Initialize GreatDocs instance.

        Parameters
        ----------
        project_path
            Path to the project root directory. Defaults to current directory.
        """
        self.project_root = Path(project_path or os.getcwd())
        # Build directory is always 'great-docs' - created during build, not init
        self.docs_dir = Path("great-docs")
        self.project_path = self.project_root / self.docs_dir
        try:
            # Python 3.9+
            self.package_path = Path(resources.files("great_docs"))
        except AttributeError:
            # Fallback for older Python versions
            import importlib_resources  # type: ignore[import-not-found]

            self.package_path = Path(importlib_resources.files("great_docs"))
        self.assets_path = self.package_path / "assets"

        # Load configuration from great-docs.yml
        self._config = Config(self._find_package_root())

    def _prepare_build_directory(self) -> None:
        """
        Prepare the great-docs/ build directory with all necessary assets.

        This method creates the great-docs/ directory (if it doesn't exist) and
        copies all CSS, JavaScript, and other assets needed for the build.
        It also creates the _quarto.yml configuration and other necessary files.

        The great-docs/ directory is ephemeral and should not be committed to
        version control. It will be recreated on each build.
        """
        print(f"Preparing build directory: {self.project_path.relative_to(self.project_root)}/")

        # Create the great-docs directory
        self.project_path.mkdir(parents=True, exist_ok=True)

        # Create necessary subdirectories
        scripts_dir = self.project_path / "scripts"
        scripts_dir.mkdir(exist_ok=True)

        reference_dir = self.project_path / "reference"
        reference_dir.mkdir(exist_ok=True)

        # Copy post-render script
        post_render_src = self.assets_path / "post-render.py"
        post_render_dst = scripts_dir / "post-render.py"
        shutil.copy2(post_render_src, post_render_dst)

        # Copy CSS file
        css_src = self.assets_path / "great-docs.css"
        css_dst = self.project_path / "great-docs.css"
        shutil.copy2(css_src, css_dst)

        # Copy JavaScript files
        js_files = [
            "github-widget.js",
            "sidebar-filter.js",
            "reference-switcher.js",
            "dark-mode-toggle.js",
            "theme-init.js",
        ]
        for js_file in js_files:
            js_src = self.assets_path / js_file
            if js_src.exists():
                js_dst = self.project_path / js_file
                shutil.copy2(js_src, js_dst)

        # Create .gitignore for the great-docs directory
        gitignore_content = """# Great Docs build directory
# This directory is ephemeral and regenerated on each build
# Do not commit this directory to version control
*
!.gitignore
"""
        gitignore_path = self.project_path / ".gitignore"
        gitignore_path.write_text(gitignore_content, encoding="utf-8")

        # Create index.qmd from README.md or user_guide files
        self._create_index_from_readme()

        # Copy user guide files if they exist in project root
        self._copy_user_guide_files()

        # Create _quarto.yml configuration
        self._update_quarto_config()

        # Add quartodoc configuration
        self._add_quartodoc_config()
        self._update_sidebar_from_sections()
        self._update_reference_index_frontmatter()

    def _copy_user_guide_files(self) -> None:
        """
        Copy user guide files from project root to build directory.

        Looks for user_guide/ directory in project root and copies
        .qmd and .md files to great-docs/user-guide/ directory.
        """
        source_user_guide = self.project_root / "user_guide"
        if not source_user_guide.exists():
            # Also check for 'user-guide' with hyphen
            source_user_guide = self.project_root / "user-guide"

        if source_user_guide.exists() and source_user_guide.is_dir():
            dest_user_guide = self.project_path / "user-guide"
            dest_user_guide.mkdir(exist_ok=True)

            # Copy all .qmd and .md files
            for pattern in ["*.qmd", "*.md"]:
                for file_path in source_user_guide.glob(pattern):
                    shutil.copy2(file_path, dest_user_guide / file_path.name)

    def install(self, force: bool = False) -> None:
        """
        Initialize great-docs in your project.

        This method creates a great-docs.yml configuration file in the project root
        with discovered exports and sensible defaults. The docs directory and assets
        will be created later during the build process.

        Parameters
        ----------
        force
            If `True`, overwrite existing great-docs.yml without prompting. Default is `False`.

        Examples
        --------
        Initialize great-docs in the current directory:

        ```python
        from great_docs import GreatDocs

        docs = GreatDocs()
        docs.install()
        ```

        Initialize in a specific project directory, overwriting existing config:

        ```python
        docs = GreatDocs("/path/to/my/project")
        docs.install(force=True)
        ```
        """
        print("Initializing great-docs...")

        # Generate great-docs.yml with discovered exports
        self._generate_initial_config(force=force)

        # Reload configuration after generating it
        self._config = Config(self._find_package_root())

        # Update project root .gitignore to exclude great-docs/
        self._update_project_gitignore(force=force)

        print("\n✅ Great Docs initialization complete!")
        print("\nNext steps:")
        print("1. Review great-docs.yml to customize your API reference structure")
        print("   (Reorder items, add sections, set 'members: false' to exclude methods)")
        print("2. Run `great-docs build` to generate and build your documentation site")
        print("3. Run `great-docs preview` to view the site locally")
        print("\nOther helpful commands:")
        print("  great-docs scan           # Preview API organization")
        print("  great-docs build --watch  # Watch for changes and rebuild")

    def _update_project_gitignore(self, force: bool = False) -> None:
        """
        Update project root .gitignore to exclude the great-docs/ build directory.

        Prompts the user for permission before modifying .gitignore, unless force=True.
        This ensures the ephemeral build directory is not committed to version control.

        Parameters
        ----------
        force
            If True, skip the prompt and automatically update .gitignore.
        """
        gitignore_path = self.project_root / ".gitignore"

        # Entry to add
        entry = "# Great Docs build directory (ephemeral, do not commit)\ngreat-docs/\n"

        # Check if already present
        if gitignore_path.exists():
            with open(gitignore_path, "r", encoding="utf-8") as f:
                content = f.read()

            if "great-docs/" in content:
                # Already present, no need to ask
                return

        # If force=True, skip the prompt
        if force:
            if gitignore_path.exists():
                # Append to existing .gitignore
                with open(gitignore_path, "a", encoding="utf-8") as f:
                    f.write("\n" + entry)
                print("✅ Updated .gitignore to exclude great-docs/ directory")
            else:
                # Create new .gitignore
                with open(gitignore_path, "w", encoding="utf-8") as f:
                    f.write(entry)
                print("✅ Created .gitignore to exclude great-docs/ directory")
            return

        # Ask for permission in interactive mode
        print("\nThe great-docs/ directory is ephemeral and should not be committed to git.")
        response = input("Add 'great-docs/' to .gitignore? [Y/n]: ").strip().lower()

        if response in ("", "y", "yes"):
            if gitignore_path.exists():
                # Append to existing .gitignore
                with open(gitignore_path, "a", encoding="utf-8") as f:
                    f.write("\n" + entry)
                print("✅ Updated .gitignore to exclude great-docs/ directory")
            else:
                # Create new .gitignore
                with open(gitignore_path, "w", encoding="utf-8") as f:
                    f.write(entry)
                print("✅ Created .gitignore to exclude great-docs/ directory")
        else:
            print(
                "⚠️  Skipped .gitignore update. Remember to exclude great-docs/ from version control."
            )

    def _detect_package_name(self) -> str | None:
        """
        Detect the Python package name from project structure.

        Returns
        -------
        str | None
            The detected package name, or None if not found.
        """
        # Look for pyproject.toml
        pyproject_path = self.project_root / "pyproject.toml"
        if pyproject_path.exists():
            import tomllib

            with open(pyproject_path, "rb") as f:
                try:
                    data = tomllib.load(f)
                    return data.get("project", {}).get("name")
                except Exception:
                    return None

        # Look for setup.py
        setup_py = self.project_root / "setup.py"
        if setup_py.exists():
            with open(setup_py, "r") as f:
                content = f.read()
                # Simple regex to find name="..." in setup()
                import re

                match = re.search(r'name\s*=\s*["\']([^"\']+)["\']', content)
                if match:
                    return match.group(1)

        # Look for a single Python package directory
        potential_packages = [
            d
            for d in self.project_root.iterdir()
            if d.is_dir() and (d / "__init__.py").exists() and not d.name.startswith(".")
        ]
        if len(potential_packages) == 1:
            return potential_packages[0].name

        return None

    def _normalize_package_name(self, package_name: str) -> str:
        """
        Convert a package name to its importable form.

        PyPI package names can use hyphens (e.g., 'great-docs') but Python
        imports must use underscores (e.g., 'great_docs'). This method handles
        the conversion.

        Parameters
        ----------
        package_name
            The package name (potentially with hyphens)

        Returns
        -------
        str
            The importable package name (with underscores)
        """
        return package_name.replace("-", "_")

    def _detect_module_name(self) -> str | None:
        """
        Detect the actual importable module name for the package.

        The module name may differ from the project name (e.g., project 'py-yaml12'
        might have module 'yaml12' or 'pyyaml12'). This method tries to find the
        actual module directory or stub file.

        Priority:
        1. Explicit 'module' setting in great-docs.yml
        2. .pyi stub files (for compiled extensions)
        3. pyproject.toml configuration ([tool.maturin], [tool.setuptools], etc.)
        4. Auto-discovery via _find_package_init

        Returns
        -------
        str | None
            The detected module name, or None if not found.
        """
        # First, check for explicit module name in great-docs.yml
        if self._config.module:
            return self._config.module

        package_root = self._find_package_root()

        # Check for .pyi stub files (common for PyO3/Rust/compiled extensions)
        # These indicate the actual module name for compiled packages
        pyi_files = list(package_root.glob("*.pyi"))
        if pyi_files:
            # Use the first .pyi file found (excluding __init__.pyi)
            for pyi_file in pyi_files:
                if pyi_file.stem != "__init__":
                    return pyi_file.stem

        # Check pyproject.toml for explicit package configuration
        pyproject_path = package_root / "pyproject.toml"
        if pyproject_path.exists():
            import tomllib

            try:
                with open(pyproject_path, "rb") as f:
                    data = tomllib.load(f)

                # Check [tool.maturin] for PyO3/Rust projects
                maturin = data.get("tool", {}).get("maturin", {})
                if maturin:
                    # maturin projects: module name is often in module-name or derived from Cargo.toml
                    module_name = maturin.get("module-name")
                    if module_name:
                        return module_name

                # Check [tool.setuptools.packages] for explicit package list
                setuptools = data.get("tool", {}).get("setuptools", {})
                if "packages" in setuptools:
                    packages = setuptools["packages"]
                    if packages and isinstance(packages, list):
                        # Return the first package (main package)
                        return packages[0].split("/")[-1]

                # Check [tool.setuptools.packages.find]
                find_config = setuptools.get("packages", {})
                if isinstance(find_config, dict) and "find" in find_config:
                    where = find_config["find"].get("where", ["."])
                    if isinstance(where, str):
                        where = [where]
                    for base_dir in where:
                        base_path = package_root / base_dir
                        if base_path.exists():
                            for item in base_path.iterdir():
                                if item.is_dir() and (item / "__init__.py").exists():
                                    if not item.name.startswith((".", "_", "test")):
                                        return item.name

                # Check [tool.hatch.build.targets.wheel.packages]
                hatch_packages = (
                    data.get("tool", {})
                    .get("hatch", {})
                    .get("build", {})
                    .get("targets", {})
                    .get("wheel", {})
                    .get("packages", [])
                )
                if hatch_packages:
                    pkg = hatch_packages[0]
                    return pkg.split("/")[-1]

            except Exception:
                pass

        # Try to find via _find_package_init
        project_name = self._detect_package_name()
        if project_name:
            init_file = self._find_package_init(project_name)
            if init_file:
                return init_file.parent.name

        return None

    def _is_compiled_extension(self) -> bool:
        """
        Check if this is a compiled extension package (PyO3, Cython, etc.).

        Returns
        -------
        bool
            True if this appears to be a compiled extension package.
        """
        package_root = self._find_package_root()

        # Check for Cargo.toml (Rust/PyO3)
        if (package_root / "Cargo.toml").exists():
            return True

        # Check for .pyi stub files without corresponding .py files
        pyi_files = list(package_root.glob("*.pyi"))
        for pyi_file in pyi_files:
            if pyi_file.stem != "__init__":
                py_file = pyi_file.with_suffix(".py")
                if not py_file.exists():
                    return True

        return False

    def _find_package_root(self) -> Path:
        """
        Find the actual package root directory (where pyproject.toml or setup.py exists).

        When the docs directory is the current directory, project_root might point to
        the docs dir rather than the package root. This method searches upward to find
        the actual package root.

        Returns
        -------
        Path
            The package root directory
        """
        current = self.project_root

        # Search upward from current directory
        for _ in range(5):  # Limit search to 5 levels up
            if (current / "pyproject.toml").exists() or (current / "setup.py").exists():
                return current
            parent = current.parent
            if parent == current:  # Reached filesystem root
                break
            current = parent

        # Fallback to project_root if we can't find it
        return self.project_root

    def _get_quarto_env(self) -> dict[str, str]:
        """
        Get environment variables for running Quarto commands.

        Sets QUARTO_PYTHON to ensure Quarto uses the correct Python environment
        where the package and its dependencies are installed. This is essential
        for notebook execution and quartodoc introspection.

        Also sets PYTHONPATH to include the package root directory, ensuring that
        griffe and quartodoc can find the package even if it's not installed.

        The method looks for Python in the following order:
        1. Virtual environment in the project root (.venv/bin/python or .venv/Scripts/python.exe)
        2. The currently running Python interpreter

        Returns
        -------
        dict[str, str]
            Environment variables dictionary with QUARTO_PYTHON and PYTHONPATH set.
        """
        import sys

        env = os.environ.copy()
        package_root = self._find_package_root()

        # Look for a virtual environment in the project
        venv_paths = [
            package_root / ".venv" / "bin" / "python",  # Unix
            package_root / ".venv" / "Scripts" / "python.exe",  # Windows
            package_root / "venv" / "bin" / "python",  # Alternative name
            package_root / "venv" / "Scripts" / "python.exe",  # Windows alternative
        ]

        python_path = None
        for venv_python in venv_paths:
            if venv_python.exists():
                python_path = str(venv_python)
                break

        # Fallback to the currently running Python
        if python_path is None:
            python_path = sys.executable

        env["QUARTO_PYTHON"] = python_path

        # Add package root to PYTHONPATH so griffe/quartodoc can find the package
        # even if it's not installed (e.g., during development)
        pythonpath_dirs = [str(package_root)]

        # Also add src/ directory if it exists (common src-layout)
        src_dir = package_root / "src"
        if src_dir.exists() and src_dir.is_dir():
            pythonpath_dirs.append(str(src_dir))

        # Also add python/ directory if it exists (common for Rust/PyO3 projects)
        python_dir = package_root / "python"
        if python_dir.exists() and python_dir.is_dir():
            pythonpath_dirs.append(str(python_dir))

        existing_pythonpath = env.get("PYTHONPATH", "")
        new_pythonpath = os.pathsep.join(pythonpath_dirs)
        if existing_pythonpath:
            # Prepend our paths to existing PYTHONPATH
            env["PYTHONPATH"] = f"{new_pythonpath}{os.pathsep}{existing_pythonpath}"
        else:
            env["PYTHONPATH"] = new_pythonpath

        return env

    def _get_package_metadata(self) -> dict:
        """
        Extract package metadata from pyproject.toml and great-docs configuration.

        Reads project metadata (license, authors, URLs, etc.) from pyproject.toml
        and Great Docs configuration from great-docs.yml.

        Returns
        -------
        dict
            Dictionary containing package metadata and great-docs configuration.
        """
        metadata = {}
        package_root = self._find_package_root()
        pyproject_path = package_root / "pyproject.toml"

        # Read project metadata from pyproject.toml
        if pyproject_path.exists():
            import tomllib

            try:
                with open(pyproject_path, "rb") as f:
                    data = tomllib.load(f)
                    project = data.get("project", {})

                    # Extract relevant fields from [project] section
                    metadata["license"] = project.get("license", {}).get("text") or project.get(
                        "license", {}
                    ).get("file", "")
                    metadata["authors"] = project.get("authors", [])
                    metadata["maintainers"] = project.get("maintainers", [])
                    metadata["urls"] = project.get("urls", {})
                    metadata["requires_python"] = project.get("requires-python", "")
                    metadata["keywords"] = project.get("keywords", [])
                    metadata["description"] = project.get("description", "")
                    metadata["optional_dependencies"] = project.get("optional-dependencies", {})

            except Exception:
                pass

        # Read Great Docs configuration from great-docs.yml
        # Reload config to ensure we have the latest
        self._config = Config(package_root)

        # Map config properties to metadata dict for backward compatibility
        metadata["rich_authors"] = self._config.authors
        metadata["exclude"] = self._config.exclude

        # Source link configuration
        metadata["source_link_enabled"] = self._config.source_enabled
        metadata["source_link_branch"] = self._config.source_branch
        metadata["source_link_path"] = self._config.source_path
        metadata["source_link_placement"] = self._config.source_placement

        # GitHub link style
        metadata["github_style"] = self._config.github_style

        # Sidebar filter configuration
        metadata["sidebar_filter_enabled"] = self._config.sidebar_filter_enabled
        metadata["sidebar_filter_min_items"] = self._config.sidebar_filter_min_items

        # CLI documentation configuration
        metadata["cli_enabled"] = self._config.cli_enabled
        metadata["cli_module"] = self._config.cli_module
        metadata["cli_name"] = self._config.cli_name

        # Dark mode toggle
        metadata["dark_mode_toggle_enabled"] = self._config.dark_mode_toggle

        return metadata

    def _update_navbar_github_link(
        self,
        config: dict,
        owner: str | None,
        repo: str | None,
        repo_url: str | None,
        github_style: str,
    ) -> None:
        """
        Update an existing navbar's GitHub link to use widget or icon style.

        Parameters
        ----------
        config
            The Quarto configuration dictionary.
        owner
            GitHub repository owner.
        repo
            GitHub repository name.
        repo_url
            Full GitHub repository URL.
        github_style
            Either "widget" (with stats dropdown) or "icon" (simple link).
        """
        if not repo_url:
            return

        navbar = config["website"]["navbar"]

        # Ensure right section exists
        if "right" not in navbar:
            navbar["right"] = []

        # Build the new GitHub entry based on style
        if github_style == "widget" and owner and repo:
            new_gh_entry = {
                "text": f'<div id="github-widget" data-owner="{owner}" data-repo="{repo}"></div>'
            }
        else:
            new_gh_entry = {"icon": "github", "href": repo_url}

        # Look for existing GitHub entry and replace it
        new_right = []
        found_github = False

        for item in navbar["right"]:
            if isinstance(item, dict):
                # Check for simple GitHub icon
                if item.get("icon") == "github":
                    new_right.append(new_gh_entry)
                    found_github = True
                # Check for existing widget
                elif "github-widget" in str(item.get("text", "")):
                    new_right.append(new_gh_entry)
                    found_github = True
                else:
                    new_right.append(item)
            else:
                new_right.append(item)

        # If no GitHub entry was found, add one
        if not found_github:
            new_right.append(new_gh_entry)

        navbar["right"] = new_right

    def _get_github_repo_info(self) -> tuple[str | None, str | None, str | None]:
        """
        Extract GitHub repository information from pyproject.toml.

        Returns
        -------
        tuple[str | None, str | None, str | None]
            A tuple of (owner, repo_name, base_url) or (None, None, None) if not found.
        """
        metadata = self._get_package_metadata()
        urls = metadata.get("urls", {})

        # Look for repository URL in various common key names
        repo_url = None
        for key in [
            "Repository",
            "repository",
            "Source",
            "source",
            "GitHub",
            "github",
            "Homepage",
            "homepage",
        ]:
            if key in urls:
                repo_url = urls[key]
                break

        if not repo_url or "github.com" not in repo_url:
            return None, None, None

        # Parse the GitHub URL to extract owner and repo
        # Handles formats like:
        # - https://github.com/owner/repo
        # - https://github.com/owner/repo.git
        # - git@github.com:owner/repo.git
        github_pattern = r"github\.com[/:]([^/]+)/([^/\s.]+)"
        match = re.search(github_pattern, repo_url)

        if match:
            owner = match.group(1)
            repo = match.group(2)
            # Remove .git suffix if present (use removesuffix, not rstrip which removes characters)
            if repo.endswith(".git"):
                repo = repo[:-4]
            base_url = f"https://github.com/{owner}/{repo}"
            return owner, repo, base_url

        return None, None, None

    # =========================================================================
    # CLI Documentation Methods
    # =========================================================================

    def _discover_click_cli(self, package_name: str) -> dict | None:
        """
        Discover Click CLI commands and groups from a package.

        Attempts to find and import the Click CLI from the package, then extracts
        command structure, help text, options, and arguments.

        Parameters
        ----------
        package_name
            The name of the package to search for CLI.

        Returns
        -------
        dict | None
            Dictionary containing CLI structure, or None if no Click CLI found.
            Structure: {
                "name": "cli-name",
                "help": "CLI help text",
                "commands": [...],  # List of command dicts
                "options": [...],   # Global options
            }
        """
        metadata = self._get_package_metadata()

        # Check if CLI documentation is enabled
        if not metadata.get("cli_enabled", False):
            return None

        try:
            import click
        except ImportError:
            print("Click not installed, skipping CLI documentation")
            return None

        # Determine the CLI module to import
        cli_module_path = metadata.get("cli_module")
        if not cli_module_path:
            # Try common CLI module locations
            common_cli_modules = [
                f"{package_name}.cli",
                f"{package_name}.__main__",
                f"{package_name}.main",
                f"{package_name}.console",
                f"{package_name}.commands",
            ]
            cli_module_path = None
            for module_path in common_cli_modules:
                try:
                    import importlib

                    module = importlib.import_module(module_path)
                    # Look for Click command/group in module
                    for attr_name in dir(module):
                        attr = getattr(module, attr_name)
                        if isinstance(attr, (click.Command, click.Group)):
                            cli_module_path = module_path
                            break
                    if cli_module_path:
                        break
                except ImportError:
                    continue

        if not cli_module_path:
            print(f"No Click CLI found in {package_name}")
            return None

        try:
            import importlib

            module = importlib.import_module(cli_module_path)
        except ImportError as e:
            print(f"Could not import CLI module {cli_module_path}: {e}")
            return None

        # Find the main Click command/group
        cli_obj = None
        cli_name = metadata.get("cli_name")

        # First, look for explicitly named CLI
        if cli_name:
            cli_obj = getattr(module, cli_name, None)

        # Otherwise, search for Click commands/groups
        if not cli_obj:
            for attr_name in ["cli", "main", "app", "command", package_name.replace("-", "_")]:
                attr = getattr(module, attr_name, None)
                if isinstance(attr, (click.Command, click.Group)):
                    cli_obj = attr
                    cli_name = attr_name
                    break

        # If still not found, look for any Click command/group
        if not cli_obj:
            for attr_name in dir(module):
                if attr_name.startswith("_"):
                    continue
                attr = getattr(module, attr_name)
                if isinstance(attr, (click.Command, click.Group)):
                    cli_obj = attr
                    cli_name = attr_name
                    break

        if not cli_obj:
            print(f"No Click command/group found in {cli_module_path}")
            return None

        print(f"Found Click CLI: {cli_name} in {cli_module_path}")

        # Get the entry point name from pyproject.toml
        entry_point_name = self._get_cli_entry_point_name(package_name)
        display_name = entry_point_name or package_name.replace("_", "-")

        # Extract CLI structure
        cli_info = self._extract_click_command(cli_obj, display_name)
        cli_info["entry_point_name"] = display_name
        return cli_info

    def _get_cli_entry_point_name(self, package_name: str) -> str | None:
        """
        Get the CLI entry point name from pyproject.toml.

        Parameters
        ----------
        package_name
            The name of the package.

        Returns
        -------
        str | None
            The entry point name (e.g., "great-docs" from "[project.scripts]"),
            or None if not found.
        """
        package_root = self._find_package_root()
        pyproject_path = package_root / "pyproject.toml"

        if not pyproject_path.exists():
            return None

        import tomllib

        try:
            with open(pyproject_path, "rb") as f:
                data = tomllib.load(f)

            # Look for [project.scripts]
            scripts = data.get("project", {}).get("scripts", {})
            if scripts:
                # Return the first entry point name
                return list(scripts.keys())[0]

            # Also check [project.gui-scripts] for GUI apps
            gui_scripts = data.get("project", {}).get("gui-scripts", {})
            if gui_scripts:
                return list(gui_scripts.keys())[0]

        except Exception:
            pass

        return None

    def _extract_click_command(
        self, cmd: "click.Command", name: str, parent_path: str = ""
    ) -> dict:
        """
        Extract information from a Click command or group.

        Parameters
        ----------
        cmd
            The Click Command or Group object.
        name
            The command name.
        parent_path
            The parent command path for nested commands.

        Returns
        -------
        dict
            Dictionary containing command information.
        """
        import click

        full_path = f"{parent_path} {name}".strip() if parent_path else name

        # Get the actual --help output from Click
        help_text = self._get_click_help_text(cmd, full_path)

        info = {
            "name": name,
            "full_path": full_path,
            "help": cmd.help or "",
            "short_help": getattr(cmd, "short_help", "") or "",
            "help_text": help_text,  # The actual --help output
            "deprecated": getattr(cmd, "deprecated", False),
            "hidden": getattr(cmd, "hidden", False),
            "commands": [],
            "is_group": isinstance(cmd, click.Group),
        }

        # Extract subcommands if this is a group
        if isinstance(cmd, click.Group):
            for subcmd_name, subcmd in cmd.commands.items():
                if not getattr(subcmd, "hidden", False):
                    subcmd_info = self._extract_click_command(subcmd, subcmd_name, full_path)
                    info["commands"].append(subcmd_info)

        return info

    def _get_click_help_text(self, cmd: "click.Command", full_path: str) -> str:
        """
        Get the formatted --help output from a Click command.

        Parameters
        ----------
        cmd
            The Click Command object.
        full_path
            The full command path (e.g., "great-docs build").

        Returns
        -------
        str
            The formatted help text as it would appear from --help.
        """
        import click

        # Create a context to get the help text
        ctx = click.Context(cmd, info_name=full_path)
        return cmd.get_help(ctx)

    def _generate_cli_reference_pages(self, cli_info: dict) -> list[str]:
        """
        Generate Quarto reference pages for CLI commands.

        Parameters
        ----------
        cli_info
            Dictionary containing CLI structure from _discover_click_cli.

        Returns
        -------
        list[str]
            List of generated .qmd file paths (relative to docs dir).
        """
        if not cli_info:
            return []

        cli_ref_dir = self.project_path / "reference" / "cli"
        cli_ref_dir.mkdir(parents=True, exist_ok=True)

        generated_files = []

        # Generate main CLI page
        main_page = self._generate_cli_command_page(cli_info, is_main=True)
        main_path = cli_ref_dir / "index.qmd"
        with open(main_path, "w") as f:
            f.write(main_page)
        generated_files.append("reference/cli/index.qmd")
        print(f"Generated CLI reference: {main_path.relative_to(self.project_path)}")

        # Generate pages for subcommands
        generated_files.extend(self._generate_subcommand_pages(cli_info, cli_ref_dir))

        return generated_files

    def _generate_subcommand_pages(self, cmd_info: dict, output_dir: Path) -> list[str]:
        """
        Recursively generate pages for subcommands.

        Parameters
        ----------
        cmd_info
            Command information dictionary.
        output_dir
            Directory to write pages to.

        Returns
        -------
        list[str]
            List of generated file paths.
        """
        generated = []

        for subcmd in cmd_info.get("commands", []):
            # Generate page for this subcommand
            page_content = self._generate_cli_command_page(subcmd, is_main=False)
            safe_name = subcmd["name"].replace("-", "_")
            page_path = output_dir / f"{safe_name}.qmd"

            with open(page_path, "w") as f:
                f.write(page_content)

            rel_path = f"reference/cli/{safe_name}.qmd"
            generated.append(rel_path)
            print(f"Generated CLI reference: {page_path.relative_to(self.project_path)}")

            # Recursively generate for nested subcommands
            if subcmd.get("commands"):
                subcmd_dir = output_dir / safe_name
                subcmd_dir.mkdir(exist_ok=True)
                generated.extend(self._generate_subcommand_pages(subcmd, subcmd_dir))

        return generated

    def _generate_cli_command_page(self, cmd_info: dict, is_main: bool = False) -> str:
        """
        Generate Quarto page content for a CLI command showing --help output.

        Parameters
        ----------
        cmd_info
            Command information dictionary.
        is_main
            Whether this is the main CLI entry point.

        Returns
        -------
        str
            Quarto markdown content with the CLI help output.
        """
        lines = []

        # Front matter: use just the command name/path as title
        title = cmd_info["full_path"] if not is_main else cmd_info["name"]
        lines.append("---")
        lines.append(f'title: "{title}"')
        lines.append("sidebar: cli-reference")
        lines.append("---")
        lines.append("")

        # Output the help text in a styled div
        lines.append("::: {.cli-manpage}")
        lines.append("")
        lines.append("```")
        lines.append(cmd_info.get("help_text", "").rstrip())
        lines.append("```")
        lines.append("")
        lines.append(":::")
        lines.append("")

        return "\n".join(lines)

    def _update_sidebar_with_cli(self, cli_files: list[str]) -> None:
        """
        Update the sidebar configuration to include CLI reference.

        Parameters
        ----------
        cli_files
            List of generated CLI reference file paths.
        """
        if not cli_files:
            return

        quarto_yml = self.project_path / "_quarto.yml"
        if not quarto_yml.exists():
            return

        with open(quarto_yml, "r") as f:
            config = yaml.safe_load(f) or {}

        if "website" not in config:
            config["website"] = {}

        # Ensure sidebar exists
        if "sidebar" not in config["website"]:
            config["website"]["sidebar"] = []

        # Check if CLI section already exists
        sidebar = config["website"]["sidebar"]
        cli_section_exists = False

        for section in sidebar:
            if isinstance(section, dict) and section.get("id") == "cli-reference":
                cli_section_exists = True
                # Update contents
                section["contents"] = cli_files
                break

        if not cli_section_exists:
            # Add CLI section
            cli_section = {
                "id": "cli-reference",
                "title": "CLI Reference",
                "contents": cli_files,
            }
            sidebar.append(cli_section)

        # Ensure the reference sidebar has an API link at the top
        for section in sidebar:
            if isinstance(section, dict) and section.get("id") == "reference":
                contents = section.get("contents", [])
                # Check if API link already exists at the top
                has_api_link = False
                if contents and isinstance(contents[0], dict):
                    if contents[0].get("text") == "API" or contents[0].get("href", "").endswith(
                        "reference/index.qmd"
                    ):
                        has_api_link = True
                if not has_api_link:
                    # Add API link at the top
                    section["contents"] = [
                        {"text": "API", "href": "reference/index.qmd"},
                    ] + contents
                break

        self._write_quarto_yml(quarto_yml, config)

        print(f"Updated sidebar with {len(cli_files)} CLI reference page(s)")

    # =========================================================================
    # User Guide Methods
    # =========================================================================

    def _discover_user_guide(self) -> dict | None:
        """
        Discover user guide content from the user_guide directory.

        Looks for a `user_guide/` directory in the project root and discovers
        all .qmd files within it. Files are sorted by filename to support
        ordering via prefixes like `00-intro.qmd`, `01-installation.qmd`.

        Returns
        -------
        dict | None
            Dictionary containing user guide structure, or None if no user guide found.
            Structure: {
                "files": [{"path": Path, "section": str | None, "title": str}, ...],
                "sections": {"Section Name": [file_info, ...], ...},
                "has_index": bool
            }
        """
        # Look for user_guide directory in project root (not docs dir)
        user_guide_dir = self._find_package_root() / "user_guide"

        if not user_guide_dir.exists() or not user_guide_dir.is_dir():
            return None

        # Find all .qmd files (not in subdirectories that are likely asset folders)
        qmd_files = []
        for item in user_guide_dir.iterdir():
            if item.is_file() and item.suffix == ".qmd":
                qmd_files.append(item)
            elif item.is_dir():
                # Check subdirectories for .qmd files (but not nested deeper)
                for subitem in item.iterdir():
                    if subitem.is_file() and subitem.suffix == ".qmd":
                        qmd_files.append(subitem)

        if not qmd_files:
            return None

        # Sort files by name to respect ordering prefixes
        qmd_files.sort(key=lambda p: p.name)

        # Parse each file to extract section and title from frontmatter
        files_info = []
        sections: dict[str, list] = {}
        has_index = False

        for qmd_path in qmd_files:
            file_info = self._parse_user_guide_file(qmd_path)
            if file_info:
                files_info.append(file_info)

                # Track if there's an index.qmd
                if qmd_path.name == "index.qmd":
                    has_index = True

                # Group by section
                section_name = file_info.get("section")
                if section_name:
                    if section_name not in sections:
                        sections[section_name] = []
                    sections[section_name].append(file_info)

        if not files_info:
            return None

        return {
            "files": files_info,
            "sections": sections,
            "has_index": has_index,
            "source_dir": user_guide_dir,
        }

    def _parse_user_guide_file(self, qmd_path: Path) -> dict | None:
        """
        Parse a user guide .qmd file to extract metadata from frontmatter.

        Parameters
        ----------
        qmd_path
            Path to the .qmd file.

        Returns
        -------
        dict | None
            Dictionary with file info: {"path": Path, "section": str | None, "title": str}
        """
        try:
            with open(qmd_path, "r", encoding="utf-8") as f:
                content = f.read()
        except Exception:
            return None

        # Extract YAML frontmatter
        frontmatter = {}
        if content.startswith("---"):
            parts = content.split("---", 2)
            if len(parts) >= 3:
                try:
                    frontmatter = yaml.safe_load(parts[1]) or {}
                except yaml.YAMLError:
                    pass

        # Get title from frontmatter or derive from filename
        title = frontmatter.get("title")
        if not title:
            # Derive title from filename: "01-getting-started.qmd" -> "Getting Started"
            name = qmd_path.stem
            # Remove leading number prefix like "01-" or "00_"
            name = re.sub(r"^\d+[-_]", "", name)
            # Convert to title case
            title = name.replace("-", " ").replace("_", " ").title()

        # Get section from frontmatter (use 'guide-section' to avoid conflict with Quarto's 'section')
        section = frontmatter.get("guide-section")

        return {
            "path": qmd_path,
            "section": section,
            "title": title,
            "frontmatter": frontmatter,
        }

    def _copy_user_guide_to_docs(self, user_guide_info: dict) -> list[str]:
        """
        Copy user guide files from project root to docs directory.

        Adds `bread-crumbs: false` to the frontmatter of each file to disable
        breadcrumb navigation on user guide pages.

        Parameters
        ----------
        user_guide_info
            User guide structure from _discover_user_guide.

        Returns
        -------
        list[str]
            List of copied file paths relative to docs dir.
        """
        if not user_guide_info:
            return []

        source_dir = user_guide_info["source_dir"]
        target_dir = self.project_path / "user-guide"
        target_dir.mkdir(parents=True, exist_ok=True)

        copied_files = []

        # Copy all .qmd files, adding bread-crumbs: false to frontmatter
        for file_info in user_guide_info["files"]:
            src_path = file_info["path"]

            # Determine relative path from source_dir
            try:
                rel_path = src_path.relative_to(source_dir)
            except ValueError:
                rel_path = Path(src_path.name)

            dst_path = target_dir / rel_path
            dst_path.parent.mkdir(parents=True, exist_ok=True)

            # Read the source file and modify frontmatter
            with open(src_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Add bread-crumbs: false to frontmatter
            content = self._add_frontmatter_option(content, "bread-crumbs", False)

            # Write to destination
            with open(dst_path, "w", encoding="utf-8") as f:
                f.write(content)

            copied_files.append(f"user-guide/{rel_path}")

        # Also copy any asset directories (directories without .qmd files)
        for item in source_dir.iterdir():
            if item.is_dir():
                # Check if this directory has any .qmd files
                has_qmd = any(f.suffix == ".qmd" for f in item.rglob("*"))
                if not has_qmd:
                    # This is likely an asset directory, copy it
                    dst_dir = target_dir / item.name
                    if dst_dir.exists():
                        shutil.rmtree(dst_dir)
                    shutil.copytree(item, dst_dir)

        return copied_files

    def _add_frontmatter_option(self, content: str, key: str, value) -> str:
        """
        Add or update an option in the YAML frontmatter of a .qmd file.

        Parameters
        ----------
        content
            The file content.
        key
            The frontmatter key to add/update.
        value
            The value to set.

        Returns
        -------
        str
            The modified content with the frontmatter option added.
        """
        # Convert value to YAML string representation
        if isinstance(value, bool):
            yaml_value = "true" if value else "false"
        elif isinstance(value, str):
            yaml_value = f'"{value}"'
        else:
            yaml_value = str(value)

        if content.startswith("---"):
            parts = content.split("---", 2)
            if len(parts) >= 3:
                frontmatter = parts[1]
                rest = parts[2]

                # Check if the key already exists
                key_pattern = rf"^{re.escape(key)}:\s*.*$"
                if re.search(key_pattern, frontmatter, re.MULTILINE):
                    # Update existing key
                    frontmatter = re.sub(
                        key_pattern, f"{key}: {yaml_value}", frontmatter, flags=re.MULTILINE
                    )
                else:
                    # Add new key at the end of frontmatter
                    frontmatter = frontmatter.rstrip() + f"\n{key}: {yaml_value}\n"

                return f"---{frontmatter}---{rest}"

        # No frontmatter, create one
        return f"---\n{key}: {yaml_value}\n---\n\n{content}"

    def _generate_user_guide_sidebar(self, user_guide_info: dict) -> dict:
        """
        Generate sidebar configuration for the user guide.

        Parameters
        ----------
        user_guide_info
            User guide structure from _discover_user_guide.

        Returns
        -------
        dict
            Sidebar configuration dict for Quarto.
        """
        source_dir = user_guide_info["source_dir"]
        files_info = user_guide_info["files"]
        sections = user_guide_info["sections"]

        contents = []

        # If we have sections, organize by section
        if sections:
            # Track which files have been assigned to sections
            assigned_files = set()

            # First, preserve section order based on first file appearance
            section_order = []
            for file_info in files_info:
                section = file_info.get("section")
                if section and section not in section_order:
                    section_order.append(section)

            # Build section entries
            for section_name in section_order:
                section_files = sections[section_name]
                section_contents = []

                for file_info in section_files:
                    rel_path = file_info["path"].relative_to(source_dir)
                    href = f"user-guide/{rel_path}"
                    assigned_files.add(file_info["path"])

                    # Use custom text for index.qmd if it has a title
                    if file_info["path"].name == "index.qmd":
                        section_contents.append(
                            {
                                "text": file_info["title"],
                                "href": href,
                            }
                        )
                    else:
                        section_contents.append(href)

                contents.append(
                    {
                        "section": section_name,
                        "contents": section_contents,
                    }
                )

            # Add any files without sections at the end
            unsectioned = []
            for file_info in files_info:
                if file_info["path"] not in assigned_files:
                    rel_path = file_info["path"].relative_to(source_dir)
                    unsectioned.append(f"user-guide/{rel_path}")

            if unsectioned:
                contents.extend(unsectioned)

        else:
            # No sections, just list files in order
            for file_info in files_info:
                rel_path = file_info["path"].relative_to(source_dir)
                contents.append(f"user-guide/{rel_path}")

        return {
            "id": "user-guide",
            "title": "User Guide",
            "contents": contents,
        }

    def _update_config_with_user_guide(self, user_guide_info: dict) -> None:
        """
        Update _quarto.yml with user guide sidebar and navbar.

        Parameters
        ----------
        user_guide_info
            User guide structure from _discover_user_guide.
        """
        quarto_yml = self.project_path / "_quarto.yml"
        if not quarto_yml.exists():
            return

        with open(quarto_yml, "r") as f:
            config = yaml.safe_load(f) or {}

        if "website" not in config:
            config["website"] = {}

        # Generate and add/update user guide sidebar
        sidebar_config = self._generate_user_guide_sidebar(user_guide_info)

        if "sidebar" not in config["website"]:
            config["website"]["sidebar"] = []

        sidebar = config["website"]["sidebar"]

        # Remove existing user-guide sidebar if present
        sidebar = [s for s in sidebar if not (isinstance(s, dict) and s.get("id") == "user-guide")]

        # Add the new user guide sidebar
        sidebar.append(sidebar_config)
        config["website"]["sidebar"] = sidebar

        # Update navbar to include User Guide link
        if "navbar" in config["website"]:
            navbar = config["website"]["navbar"]
            if "left" in navbar:
                # Check if User Guide link already exists
                has_user_guide = any(
                    isinstance(item, dict) and item.get("text") == "User Guide"
                    for item in navbar["left"]
                )
                if not has_user_guide:
                    # Find the position after "Home" and before "Reference"
                    insert_idx = 1  # Default: after Home
                    for i, item in enumerate(navbar["left"]):
                        if isinstance(item, dict) and item.get("text") == "Reference":
                            insert_idx = i
                            break

                    # Determine the href for User Guide
                    if user_guide_info.get("has_index"):
                        user_guide_href = "user-guide/index.qmd"
                    else:
                        # Use the first file
                        first_file = user_guide_info["files"][0]
                        rel_path = first_file["path"].relative_to(user_guide_info["source_dir"])
                        user_guide_href = f"user-guide/{rel_path}"

                    navbar["left"].insert(
                        insert_idx,
                        {
                            "text": "User Guide",
                            "href": user_guide_href,
                        },
                    )

        self._write_quarto_yml(quarto_yml, config)

    def _process_user_guide(self) -> bool:
        """
        Process user guide content: discover, copy, and update configuration.

        Returns
        -------
        bool
            True if user guide was processed, False otherwise.
        """
        # Discover user guide
        user_guide_info = self._discover_user_guide()
        if not user_guide_info:
            return False

        print("\n📖 Processing User Guide...")
        print(f"   Found {len(user_guide_info['files'])} page(s)")

        # Copy files to docs directory
        copied_files = self._copy_user_guide_to_docs(user_guide_info)
        print(f"   Copied {len(copied_files)} file(s) to docs/user-guide/")

        # Update configuration
        self._update_config_with_user_guide(user_guide_info)

        # Report sections
        if user_guide_info["sections"]:
            section_names = list(user_guide_info["sections"].keys())
            print(f"   Sections: {', '.join(section_names)}")

        print("✅ User Guide configured")
        return True

    def _get_source_location(self, package_name: str, item_name: str) -> dict | None:
        """
        Get source file and line numbers for a class, method, or function.

        Uses griffe for static analysis to avoid import side effects.

        Parameters
        ----------
        package_name
            The name of the package containing the item.
        item_name
            The fully qualified name of the item (e.g., "ClassName" or "ClassName.method_name").

        Returns
        -------
        dict | None
            Dictionary with file path and line numbers, or None if not found.
        """
        try:
            import griffe

            normalized_name = package_name.replace("-", "_")

            # Load the package with griffe
            try:
                pkg = griffe.load(normalized_name)
            except Exception:
                return None

            # Navigate to the item (handle dotted names like "ClassName.method")
            parts = item_name.split(".")
            obj = pkg

            for part in parts:
                if part not in obj.members:
                    return None
                obj = obj.members[part]

            # Get source information
            if not hasattr(obj, "lineno") or obj.lineno is None:
                return None

            # Get the file path relative to package root
            if hasattr(obj, "filepath") and obj.filepath:
                filepath = str(obj.filepath)
            else:
                return None

            # Get end line number
            end_lineno = getattr(obj, "endlineno", obj.lineno)

            return {
                "file": filepath,
                "start_line": obj.lineno,
                "end_line": end_lineno or obj.lineno,
            }

        except ImportError:
            return None
        except Exception:
            return None

    def _build_github_source_url(
        self, source_location: dict, branch: str | None = None
    ) -> str | None:
        """
        Build a GitHub URL for viewing source code at specific line numbers.

        Parameters
        ----------
        source_location
            Dictionary with file path and line numbers.
        branch
            Git branch/tag to link to. If None, attempts to detect from git
            or falls back to 'main'.

        Returns
        -------
        str | None
            Full GitHub URL with line anchors, or None if repo info not available.
        """
        owner, repo, base_url = self._get_github_repo_info()

        if not base_url:
            return None

        # Determine the branch/ref to use
        if branch is None:
            branch = self._detect_git_ref()

        # Get the file path relative to the repository root
        filepath = source_location.get("file", "")
        package_root = self._find_package_root()

        # Handle source path configuration for monorepos
        metadata = self._get_package_metadata()
        source_path = metadata.get("source_link_path")

        if source_path:
            # Custom source path specified (for monorepos)
            relative_path = f"{source_path}/{Path(filepath).name}"
        else:
            # Try to make the path relative to package root
            try:
                filepath_obj = Path(filepath)
                if filepath_obj.is_absolute():
                    relative_path = str(filepath_obj.relative_to(package_root))
                else:
                    relative_path = filepath
            except ValueError:
                # Path is not relative to package root, use as-is
                relative_path = filepath

        # Build the URL with line number anchors
        start_line = source_location.get("start_line", 1)
        end_line = source_location.get("end_line", start_line)

        url = f"{base_url}/blob/{branch}/{relative_path}"

        if start_line == end_line:
            url += f"#L{start_line}"
        else:
            url += f"#L{start_line}-L{end_line}"

        return url

    def _detect_git_ref(self) -> str:
        """
        Detect the current git branch or tag.

        Returns
        -------
        str
            The current branch/tag name, or 'main' as fallback.
        """
        import subprocess

        package_root = self._find_package_root()

        # First check if there's a configured branch in metadata
        metadata = self._get_package_metadata()
        configured_branch = metadata.get("source_link_branch")
        if configured_branch:
            return configured_branch

        try:
            # Try to get the current tag first (for versioned docs)
            result = subprocess.run(
                ["git", "describe", "--tags", "--exact-match"],
                cwd=package_root,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                return result.stdout.strip()

            # Fall back to branch name
            result = subprocess.run(
                ["git", "rev-parse", "--abbrev-ref", "HEAD"],
                cwd=package_root,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                branch = result.stdout.strip()
                if branch != "HEAD":
                    return branch

        except Exception:
            pass

        # Default fallback
        return "main"

    def _generate_source_links_json(self, package_name: str) -> None:
        """
        Generate a JSON file mapping object names to their GitHub source URLs.

        This file is used by the post-render script to inject source links
        into the HTML documentation.

        Parameters
        ----------
        package_name
            The name of the package to generate source links for.
        """
        import json

        metadata = self._get_package_metadata()

        # Check if source links are enabled
        if not metadata.get("source_link_enabled", True):
            print("Source links disabled in configuration")
            return

        # Check if we have GitHub repo info
        owner, repo, base_url = self._get_github_repo_info()
        if not base_url:
            print("No GitHub repository URL found, skipping source links")
            return

        print(f"Generating source links for {package_name}...")

        source_links: dict[str, dict] = {}
        normalized_name = package_name.replace("-", "_")

        # Get all exports
        exports = self._get_package_exports(package_name)
        if not exports:
            return

        # Get branch for source links
        branch = self._detect_git_ref()
        print(f"Using git ref: {branch}")

        # Generate source links for each export
        for item_name in exports:
            source_loc = self._get_source_location(normalized_name, item_name)
            if source_loc:
                github_url = self._build_github_source_url(source_loc, branch)
                if github_url:
                    source_links[item_name] = {
                        "url": github_url,
                        "file": source_loc.get("file", ""),
                        "start_line": source_loc.get("start_line", 0),
                        "end_line": source_loc.get("end_line", 0),
                    }

            # Also get source links for methods of classes
            categories = self._categorize_api_objects(package_name, [item_name])
            if item_name in categories.get("classes", []):
                method_names = categories.get("class_method_names", {}).get(item_name, [])
                for method_name in method_names:
                    full_name = f"{item_name}.{method_name}"
                    method_loc = self._get_source_location(normalized_name, full_name)
                    if method_loc:
                        method_url = self._build_github_source_url(method_loc, branch)
                        if method_url:
                            source_links[full_name] = {
                                "url": method_url,
                                "file": method_loc.get("file", ""),
                                "start_line": method_loc.get("start_line", 0),
                                "end_line": method_loc.get("end_line", 0),
                            }

        # Write to JSON file in the docs directory
        source_links_path = self.project_path / "_source_links.json"
        with open(source_links_path, "w", encoding="utf-8") as f:
            json.dump(source_links, f, indent=2)

        print(f"Generated source links for {len(source_links)} items")

    def _find_package_init(self, package_name: str) -> Path | None:
        """
        Find the __init__.py file for a package, searching common locations.

        This handles packages with non-standard structures like Rust bindings
        that may have their Python code in subdirectories like python/, src/, etc.

        Parameters
        ----------
        package_name
            The name of the package to find.

        Returns
        -------
        Path | None
            Path to the __init__.py file, or None if not found.
        """
        # Normalize package name (replace dashes with underscores)
        normalized_name = package_name.replace("-", "_")

        # First, try to get explicit package info from pyproject.toml
        package_root = self._find_package_root()
        pyproject_path = package_root / "pyproject.toml"
        explicit_packages = []

        if pyproject_path.exists():
            import tomllib

            try:
                with open(pyproject_path, "rb") as f:
                    data = tomllib.load(f)

                # Check [tool.setuptools.packages] for explicit package list
                setuptools = data.get("tool", {}).get("setuptools", {})
                if "packages" in setuptools:
                    explicit_packages = setuptools["packages"]
                elif "packages" in setuptools.get("find", {}):
                    # [tool.setuptools.packages.find] with where
                    where = setuptools.get("find", {}).get("where", ["."])
                    if isinstance(where, str):
                        where = [where]
                    for base_dir in where:
                        base_path = package_root / base_dir
                        if base_path.exists():
                            # Look for packages in this directory
                            for item in base_path.iterdir():
                                if item.is_dir() and (item / "__init__.py").exists():
                                    if not item.name.startswith((".", "_", "test")):
                                        explicit_packages.append(item.name)

                # Check [tool.hatch.build.targets.wheel] for hatch projects
                hatch_packages = (
                    data.get("tool", {})
                    .get("hatch", {})
                    .get("build", {})
                    .get("targets", {})
                    .get("wheel", {})
                    .get("packages", [])
                )
                if hatch_packages:
                    explicit_packages.extend(hatch_packages)

            except Exception:
                pass

        # Build search paths, prioritizing explicit packages from pyproject.toml
        search_paths = []

        # Add explicit packages first (from pyproject.toml config)
        for pkg in explicit_packages:
            # Handle "src/package" style paths
            if "/" in pkg:
                search_paths.append(package_root / pkg)
            else:
                search_paths.append(package_root / pkg)
                search_paths.append(package_root / "src" / pkg)

        # Then add standard locations based on package name
        search_paths.extend(
            [
                package_root / package_name,
                package_root / normalized_name,
                package_root / "python" / package_name,
                package_root / "python" / normalized_name,
                package_root / "src" / package_name,
                package_root / "src" / normalized_name,
                package_root / "lib" / package_name,
                package_root / "lib" / normalized_name,
            ]
        )

        # First pass: look for __init__.py with __version__ or __all__
        for package_dir in search_paths:
            if not package_dir.exists() or not package_dir.is_dir():
                continue

            init_file = package_dir / "__init__.py"
            if init_file.exists():
                try:
                    with open(init_file, "r", encoding="utf-8") as f:
                        content = f.read()
                        if "__version__" in content or "__all__" in content:
                            return init_file
                except Exception:
                    continue

        # Second pass: accept any __init__.py in a matching directory
        for package_dir in search_paths:
            if not package_dir.exists() or not package_dir.is_dir():
                continue

            init_file = package_dir / "__init__.py"
            if init_file.exists():
                return init_file

        # Third pass: auto-discover any Python package in common locations
        # This handles cases where the package name doesn't match the project name
        auto_discover_dirs = [
            package_root,
            package_root / "src",
            package_root / "python",
            package_root / "lib",
        ]

        for base_dir in auto_discover_dirs:
            if not base_dir.exists():
                continue
            for item in base_dir.iterdir():
                if not item.is_dir():
                    continue
                # Skip common non-package directories
                if item.name.startswith((".", "_")) or item.name in (
                    "tests",
                    "test",
                    "docs",
                    "doc",
                    "examples",
                    "scripts",
                    "build",
                    "dist",
                    "__pycache__",
                    "venv",
                    ".venv",
                    "node_modules",
                    "site-packages",
                ):
                    continue
                init_file = item / "__init__.py"
                if init_file.exists():
                    return init_file

        return None

    def _parse_package_exports(self, package_name: str) -> list | None:
        """
        Parse __all__ from package's __init__.py to get public API.

        Also checks for __gt_exclude__ in __init__.py or exclude in great-docs.yml
        to filter out non-documentable items.

        Parameters
        ----------
        package_name
            The name of the package to parse.

        Returns
        -------
        list | None
            List of public names from __all__ (filtered by exclusions), or None if not found.
        """
        # Find the package's __init__.py file
        init_file = self._find_package_init(package_name)
        if not init_file:
            # Show both the original and normalized name for clarity
            normalized = package_name.replace("-", "_")
            if normalized != package_name:
                print(
                    f"Could not locate __init__.py for package '{package_name}' (module name: '{normalized}')"
                )
                print(f"Tip: Ensure a '{normalized}/' directory exists with an __init__.py file")
            else:
                print(f"Could not locate __init__.py for package '{package_name}'")
            return None

        print(f"Found package __init__.py at: {init_file.relative_to(self.project_root)}")

        # Get exclusions from great-docs.yml
        metadata = self._get_package_metadata()
        config_exclude = metadata.get("exclude", [])

        try:
            with open(init_file, "r", encoding="utf-8") as f:
                content = f.read()

            # Try to extract __all__ and __gt_exclude__ using AST (safer than eval)
            import ast

            tree = ast.parse(content)

            all_exports = None
            gt_exclude = []

            for node in ast.walk(tree):
                if isinstance(node, ast.Assign):
                    for target in node.targets:
                        # Extract __all__
                        if isinstance(target, ast.Name) and target.id == "__all__":
                            if isinstance(node.value, ast.List):
                                all_exports = []
                                for elt in node.value.elts:
                                    if isinstance(elt, ast.Constant) and isinstance(elt.value, str):
                                        all_exports.append(elt.value)

                        # Extract __gt_exclude__ (legacy support)
                        if isinstance(target, ast.Name) and target.id == "__gt_exclude__":
                            if isinstance(node.value, ast.List):
                                for elt in node.value.elts:
                                    if isinstance(elt, ast.Constant) and isinstance(elt.value, str):
                                        gt_exclude.append(elt.value)

            if all_exports:
                print(f"Successfully parsed __all__ with {len(all_exports)} exports")

                # Combine exclusions from both sources
                all_exclude = list(set(gt_exclude + config_exclude))

                # Filter out excluded items
                if all_exclude:
                    filtered = [e for e in all_exports if e not in all_exclude]
                    excluded_count = len(all_exports) - len(filtered)
                    if excluded_count > 0:
                        source = []
                        if gt_exclude:
                            source.append("__gt_exclude__")
                        if config_exclude:
                            source.append("great-docs.yml exclude")
                        print(
                            f"Filtered out {excluded_count} item(s) from {' and '.join(source)}: {', '.join(all_exclude)}"
                        )
                    return filtered
                else:
                    return all_exports

            print("No __all__ definition found in __init__.py")
            return None
        except Exception as e:
            print(f"Error parsing __all__: {type(e).__name__}: {e}")
            return None

    # Auto-excluded names that are typically not meant for documentation
    # These are common internal/utility exports that most packages don't want documented
    AUTO_EXCLUDE = {
        # CLI and entry points
        "main",  # CLI entry point function
        "cli",  # CLI module
        # Version and metadata
        "version",  # Version string/function
        "VERSION",  # Uppercase version constant
        "VERSION_INFO",  # Version info tuple
        # Common module re-exports
        "core",  # Core module
        "utils",  # Utilities module
        "helpers",  # Helpers module
        "constants",  # Constants module
        "config",  # Config module
        "settings",  # Settings module
        # Standard library re-exports
        "PackageNotFoundError",  # importlib.metadata exception
        "typing",  # typing module re-export
        "annotations",  # annotations module re-export
        "TYPE_CHECKING",  # typing.TYPE_CHECKING constant
        # Logging
        "logger",  # Module-level logger instance
        "log",  # Alternative logger name
        "logging",  # logging module re-export
    }

    def _discover_package_exports(self, package_name: str) -> list | None:
        """
        Discover public API objects using griffe introspection.

        This method uses griffe (quartodoc's introspection library) to statically analyze the
        package and discover all public objects by filtering out private/internal names (those
        starting with underscore).

        Auto-excludes common internal names (see `AUTO_EXCLUDE`). Additional items can be
        excluded via the `exclude` option in `great-docs.yml`.

        Parameters
        ----------
        package_name
            The name of the package to discover exports from.

        Returns
        -------
        list | None
            List of public names discovered (filtered by exclusions), or `None` if discovery failed.
        """
        try:
            import griffe

            # Normalize package name (replace dashes with underscores)
            normalized_name = package_name.replace("-", "_")

            # Load the package using griffe
            try:
                pkg = griffe.load(normalized_name)
            except Exception as e:
                print(f"Warning: Could not load package with griffe ({type(e).__name__})")
                if package_name != normalized_name:
                    print(
                        f"   (Looking for module '{normalized_name}' from project '{package_name}')"
                    )
                return None

            # Get all members from the package (equivalent to dir(package))
            all_members = list(pkg.members.keys())

            # Filter out private names (starting with underscore)
            # This also filters out dunder names like __version__, __all__, etc.
            public_members = [name for name in all_members if not name.startswith("_")]

            print(f"Discovered {len(public_members)} public names")

            # Get config from great-docs.yml
            metadata = self._get_package_metadata()
            config_exclude = set(metadata.get("exclude", []))

            # Apply auto-exclusions
            auto_excluded_found = [name for name in public_members if name in self.AUTO_EXCLUDE]
            if auto_excluded_found:
                print(
                    f"Auto-excluding {len(auto_excluded_found)} item(s): "
                    f"{', '.join(sorted(auto_excluded_found))}"
                )

            # Combine all exclusions (auto + user-specified)
            all_exclude = self.AUTO_EXCLUDE | config_exclude

            # Filter out excluded items
            filtered = [name for name in public_members if name not in all_exclude]

            # Report user-specified exclusions separately
            if config_exclude:
                user_excluded_found = [
                    name
                    for name in public_members
                    if name in config_exclude and name not in self.AUTO_EXCLUDE
                ]
                if user_excluded_found:
                    print(
                        f"Filtered out {len(user_excluded_found)} item(s) from great-docs.yml exclude: "
                        f"{', '.join(sorted(user_excluded_found))}"
                    )

            # Super-safe filtering: try each object with quartodoc's get_object
            # If it fails for ANY reason, exclude it; this catches:
            # - Cyclic aliases
            # - Unresolvable aliases
            # - Rust/PyO3 objects (KeyError)
            # - Submodules (which would cause recursive documentation issues)
            # - Any other edge case that would crash quartodoc build
            safe_exports = []
            failed_exports = {}  # name -> error type for reporting

            # Try to use quartodoc's `get_object()` for validation
            quartodoc_get_object = None
            try:
                from functools import partial

                from quartodoc import get_object as qd_get_object

                # quartodoc uses `parser="numpy"` by default which affects alias resolution
                quartodoc_get_object = partial(qd_get_object, dynamic=True, parser="numpy")
            except ImportError:
                pass

            # Try to import the actual package to detect modules
            actual_package = None
            try:
                import importlib

                actual_package = importlib.import_module(normalized_name)
            except ImportError:
                pass

            for name in filtered:
                # First check if this is a submodule; these should be excluded
                # because documenting them recursively documents all their members
                if actual_package is not None:
                    runtime_obj = getattr(actual_package, name, None)
                    if runtime_obj is not None:
                        import types

                        if isinstance(runtime_obj, types.ModuleType):
                            failed_exports[name] = "submodule (excluded from top-level docs)"
                            continue

                if quartodoc_get_object is not None:
                    try:
                        # Try to load the object exactly as quartodoc would
                        qd_obj = quartodoc_get_object(f"{normalized_name}:{name}")
                        # Try to access members to trigger any lazy resolution errors
                        _ = qd_obj.members
                        _ = qd_obj.kind
                        safe_exports.append(name)
                    except griffe.CyclicAliasError:
                        failed_exports[name] = "cyclic alias"
                    except griffe.AliasResolutionError:
                        failed_exports[name] = "unresolvable alias"
                    except KeyError:
                        failed_exports[name] = "not found (likely Rust/PyO3)"
                    except Exception as e:
                        # Catch-all for any other error that would crash quartodoc
                        failed_exports[name] = f"{type(e).__name__}"
                else:
                    # Fallback: use basic griffe check if quartodoc not available
                    try:
                        obj = pkg.members[name]
                        _ = obj.kind
                        _ = obj.members
                        safe_exports.append(name)
                    except Exception as e:
                        failed_exports[name] = f"{type(e).__name__}"

            # Report excluded items grouped by error type
            if failed_exports:
                # Group by error type for cleaner output
                by_error = {}
                for name, error in failed_exports.items():
                    by_error.setdefault(error, []).append(name)

                for error_type, names in sorted(by_error.items()):
                    print(
                        f"Excluding {len(names)} object(s) ({error_type}): "
                        f"{', '.join(sorted(names))}"
                    )

            return safe_exports

        except ImportError:
            print("Warning: griffe not available, cannot use dir() discovery")
            return None
        except Exception as e:
            print(f"Error discovering exports via dir(): {type(e).__name__}: {e}")
            return None

    def _detect_docstring_style(self, package_name: str) -> str:
        """
        Detect the docstring style used in a package.

        Analyzes docstrings from the package to determine if they use NumPy, Google,
        or Sphinx style formatting. The detection is based on characteristic patterns:

        - NumPy style: Uses `---` underlines under section headers (Parameters, Returns, etc.)
        - Google style: Uses section headers with colons but NO underlines (Args:, Returns:)
        - Sphinx style: Uses `:param:`, `:returns:`, etc. field markers

        Parameters
        ----------
        package_name
            The name of the package to analyze.

        Returns
        -------
        str
            The detected docstring style: "numpy", "google", or "sphinx".
            Defaults to "numpy" if detection is inconclusive.
        """
        try:
            import griffe

            # Normalize package name
            normalized_name = package_name.replace("-", "_")

            # Load the package using griffe
            try:
                pkg = griffe.load(normalized_name)
            except Exception as e:
                print(
                    f"Warning: Could not load package for docstring detection ({type(e).__name__})"
                )
                return "numpy"

            # Collect docstrings from the package
            docstrings = []

            def collect_docstrings(obj, depth=0):
                """Recursively collect docstrings from an object and its members."""
                if depth > 2:  # Limit recursion depth
                    return

                # Get the object's docstring
                if hasattr(obj, "docstring") and obj.docstring:
                    docstrings.append(obj.docstring.value)

                # Recurse into members
                if hasattr(obj, "members"):
                    for member in obj.members.values():
                        try:
                            # Skip aliases to avoid infinite loops
                            if hasattr(member, "is_alias") and member.is_alias:
                                continue
                            collect_docstrings(member, depth + 1)
                        except Exception:
                            continue

            collect_docstrings(pkg)

            if not docstrings:
                print("No docstrings found, defaulting to numpy style")
                return "numpy"

            # Analyze docstrings for style indicators
            numpy_indicators = 0
            google_indicators = 0
            sphinx_indicators = 0

            # Patterns for detection
            # NumPy: section headers followed by dashes (e.g., "Parameters\n----------")
            numpy_section_pattern = re.compile(
                r"^\s*(Parameters|Returns|Yields|Raises|Examples|Attributes|Methods|See Also|Notes|References|Warnings)\s*\n\s*-{3,}",
                re.MULTILINE,
            )

            # Google: section headers with colons (e.g., "Args:", "Returns:")
            google_section_pattern = re.compile(
                r"^\s*(Args|Arguments|Returns|Yields|Raises|Examples|Attributes|Note|Notes|Todo|Warning|Warnings):\s*$",
                re.MULTILINE,
            )

            # Sphinx: field markers (e.g., ":param name:", ":returns:")
            sphinx_pattern = re.compile(
                r"^\s*:(param|type|returns|rtype|raises|var|ivar|cvar)\s",
                re.MULTILINE,
            )

            # Example blocks with >>> are common in both NumPy and Google styles
            # but the presence/absence of --- is the key differentiator

            for docstring in docstrings:
                if not docstring:
                    continue

                # Check for NumPy style (section + dashes)
                if numpy_section_pattern.search(docstring):
                    numpy_indicators += 1

                # Check for Google style (section headers with colons, no dashes)
                if google_section_pattern.search(docstring):
                    # Only count as Google if there are NO numpy-style dashes nearby
                    if not numpy_section_pattern.search(docstring):
                        google_indicators += 1

                # Check for Sphinx style
                if sphinx_pattern.search(docstring):
                    sphinx_indicators += 1

            # Determine the winner
            total_indicators = numpy_indicators + google_indicators + sphinx_indicators

            if total_indicators == 0:
                print("No clear docstring style detected, defaulting to numpy")
                return "numpy"

            # Report findings
            print(
                f"Docstring style detection: numpy={numpy_indicators}, "
                f"google={google_indicators}, sphinx={sphinx_indicators}"
            )

            # Return the style with most indicators
            if sphinx_indicators > numpy_indicators and sphinx_indicators > google_indicators:
                print("Detected sphinx docstring style")
                return "sphinx"
            elif google_indicators > numpy_indicators:
                print("Detected google docstring style")
                return "google"
            else:
                print("Detected numpy docstring style")
                return "numpy"

        except ImportError:
            print("Warning: griffe not available for docstring detection, defaulting to numpy")
            return "numpy"
        except Exception as e:
            print(f"Error detecting docstring style: {type(e).__name__}: {e}")
            return "numpy"

    def _detect_dynamic_mode(self, package_name: str) -> bool:
        """
        Detect whether dynamic introspection mode works for a package.

        Quartodoc's `dynamic: true` mode uses runtime introspection which is more
        accurate but can fail for packages with certain module structures (e.g.,
        PyO3/Rust bindings, complex re-exports) that cause cyclic alias errors.

        This method tests if dynamic mode works by attempting to load objects
        with quartodoc's get_object function in dynamic mode AND accessing their
        members (which is what triggers cyclic alias errors in some packages).

        Parameters
        ----------
        package_name
            The name of the package to test.

        Returns
        -------
        bool
            True if dynamic mode works, False if it causes errors.
        """
        try:
            import griffe
            from quartodoc import get_object as qd_get_object
        except ImportError:
            # If quartodoc isn't available, default to True (will fail at build time anyway)
            return True

        # Normalize package name
        normalized_name = package_name.replace("-", "_")

        # Get a sample of exports to test
        try:
            pkg = griffe.load(normalized_name)
            exports = [
                name
                for name in list(pkg.members.keys())[:10]  # Test first 10
                if not name.startswith("_")
            ]
        except Exception:
            # Can't load package, default to True
            return True

        if not exports:
            return True

        # Test dynamic mode with a few exports
        # We need to actually access .members and .kind to trigger cyclic alias errors
        cyclic_errors = 0
        for name in exports[:5]:  # Test up to 5 exports
            try:
                obj = qd_get_object(f"{normalized_name}:{name}", dynamic=True)
                # Access .members to trigger cyclic alias resolution
                # This is what quartodoc does when rendering docs
                _ = obj.members
                _ = obj.kind
                # For classes, try to access individual members too
                if hasattr(obj, "members") and obj.members:
                    for member_name, member in list(obj.members.items())[:3]:
                        try:
                            _ = member.kind
                        except griffe.CyclicAliasError:
                            cyclic_errors += 1
                            break
            except griffe.CyclicAliasError:
                cyclic_errors += 1
            except Exception:
                # Other errors don't necessarily mean dynamic mode won't work
                pass

        if cyclic_errors > 0:
            print(f"Detected {cyclic_errors} cyclic alias error(s), using dynamic: false")
            return False
        else:
            print("Dynamic introspection mode works for this package")
            return True

    def _get_package_exports(self, package_name: str) -> list | None:
        """
        Get package exports using static analysis.

        Uses griffe to statically analyze the package and discover public objects.
        Falls back to parsing __all__ if griffe introspection fails.

        Parameters
        ----------
        package_name
            The name of the package to get exports from.

        Returns
        -------
        list | None
            List of exported/public names, or None if discovery failed.
        """
        exports = self._discover_package_exports(package_name)
        if exports is None:
            print("Falling back to __all__ discovery")
            return self._parse_package_exports(package_name)
        return exports

    def _categorize_api_objects(self, package_name: str, exports: list) -> dict:
        """
        Categorize API objects using griffe introspection.

        Uses griffe (quartodoc's introspection library) to analyze the package
        structure without importing it. This is safer and works with packages
        that have non-Python components (e.g., Rust bindings).

        Parameters
        ----------
        package_name
            The name of the package.
        exports
            List of exported names from __all__.

        Returns
        -------
        dict
            Dictionary with:
            - classes: list of class names
            - functions: list of function names
            - other: list of other object names
            - class_methods: dict mapping class name to method count
            - class_method_names: dict mapping class name to list of method names
        """
        try:
            import griffe

            # Load the package using griffe
            normalized_name = package_name.replace("-", "_")

            # Try to use quartodoc's `get_object()` for validation
            quartodoc_get_object = None
            try:
                from functools import partial

                from quartodoc import get_object as qd_get_object

                quartodoc_get_object = partial(qd_get_object, dynamic=True, parser="numpy")
            except ImportError:
                pass

            # Try to load the package with griffe
            try:
                pkg = griffe.load(normalized_name)
            except Exception as e:
                print(f"Warning: Could not load package with griffe ({type(e).__name__})")
                # Fallback to simple categorization
                skip_names = {"__version__", "__author__", "__email__", "__all__"}
                filtered_exports = [e for e in exports if e not in skip_names]
                return {
                    "classes": [],
                    "functions": [],
                    "other": filtered_exports,
                    "class_methods": {},
                    "class_method_names": {},
                    "cyclic_alias_count": 0,
                }

            categories = {
                "classes": [],
                "functions": [],
                "other": [],
                "class_methods": {},
                "class_method_names": {},
                "cyclic_alias_count": 0,
            }
            failed_introspection = []
            cyclic_aliases = []

            # Skip common metadata variables
            skip_names = {"__version__", "__author__", "__email__", "__all__"}

            for name in exports:
                # Skip metadata variables
                if name in skip_names:
                    continue

                try:
                    # Get the object from the loaded package
                    if name not in pkg.members:
                        categories["other"].append(name)
                        failed_introspection.append(name)
                        continue

                    obj = pkg.members[name]

                    # Categorize based on griffe's kind
                    # Note: Accessing obj.kind or obj.members on an Alias can trigger
                    # resolution which may raise CyclicAliasError or AliasResolutionError
                    if obj.kind.value == "class":
                        categories["classes"].append(name)
                        # Get public methods (exclude private/magic methods)
                        # We need to handle each member individually to catch cyclic aliases
                        # AND validate each method with quartodoc to catch type hint issues
                        # Collect (method_name, lineno) tuples to preserve source order
                        method_entries = []
                        skipped_methods = []
                        try:
                            for member_name, member in obj.members.items():
                                if member_name.startswith("_"):
                                    continue
                                try:
                                    # Accessing member.kind can trigger alias resolution
                                    if member.kind.value in ("function", "method"):
                                        # Get line number for source ordering
                                        lineno = getattr(member, "lineno", float("inf"))
                                        # Validate with quartodoc if available
                                        if quartodoc_get_object is not None:
                                            try:
                                                qd_obj = quartodoc_get_object(
                                                    f"{normalized_name}:{name}.{member_name}"
                                                )
                                                # Try to access properties that might fail
                                                _ = qd_obj.members
                                                _ = qd_obj.kind
                                                method_entries.append((member_name, lineno))
                                            except Exception:
                                                # Method can't be documented by quartodoc
                                                skipped_methods.append(member_name)
                                        else:
                                            method_entries.append((member_name, lineno))
                                except (
                                    griffe.CyclicAliasError,
                                    griffe.AliasResolutionError,
                                ):
                                    # Skip cyclic/unresolvable class members
                                    skipped_methods.append(member_name)
                                except Exception:
                                    # Skip members that can't be introspected
                                    pass
                        except (griffe.CyclicAliasError, griffe.AliasResolutionError):
                            # If we can't even iterate members, class has issues
                            skipped_methods.append("<members>")

                        # Sort by line number to preserve source file order
                        method_entries.sort(key=lambda x: x[1])
                        method_names = [entry[0] for entry in method_entries]

                        if skipped_methods:
                            print(
                                f"{name}: class with {len(method_names)} public methods "
                                f"(skipped {len(skipped_methods)} undocumentable method(s): "
                                f"{', '.join(skipped_methods[:3])}{'...' if len(skipped_methods) > 3 else ''})"
                            )
                        else:
                            print(f"{name}: class with {len(method_names)} public methods")

                        categories["class_methods"][name] = len(method_names)
                        categories["class_method_names"][name] = method_names
                    elif obj.kind.value == "function":
                        categories["functions"].append(name)
                    else:
                        # Attributes, modules, etc.
                        categories["other"].append(name)

                except griffe.CyclicAliasError:
                    # Cyclic alias detected (e.g., re-exported symbol pointing to itself)
                    # This can happen with complex re-export patterns
                    # Do NOT add to categories (these must be excluded entirely)
                    print(f"  Warning: Cyclic alias detected for '{name}', excluding from docs")
                    cyclic_aliases.append(name)
                except griffe.AliasResolutionError:
                    # Alias could not be resolved (target not found)
                    # Do NOT add to categories (these must be excluded entirely)
                    print(f"  Warning: Could not resolve alias for '{name}', excluding from docs")
                    failed_introspection.append(name)
                except Exception as e:
                    # If introspection fails for a specific object, still include it
                    print(f"  Warning: Could not introspect '{name}': {type(e).__name__}")
                    categories["other"].append(name)
                    failed_introspection.append(name)

            if cyclic_aliases:
                print(f"Note: Excluded {len(cyclic_aliases)} cyclic alias(es) from documentation")
                categories["cyclic_alias_count"] = len(cyclic_aliases)

            if failed_introspection:
                print(
                    f"Note: Could not introspect {len(failed_introspection)} item(s), categorizing as 'Other'"
                )

            return categories

        except ImportError:
            print("Warning: griffe not available, using fallback categorization")
            # Fallback if griffe isn't installed
            skip_names = {"__version__", "__author__", "__email__", "__all__"}
            filtered_exports = [e for e in exports if e not in skip_names]
            return {
                "classes": [],
                "functions": [],
                "other": filtered_exports,
                "class_methods": {},
                "class_method_names": {},
                "cyclic_alias_count": 0,
            }

    def _create_quartodoc_sections(self, package_name: str) -> list | None:
        """
        Create quartodoc sections based on discovered package exports.

        Uses static analysis (griffe) to discover public objects.

        Uses this heuristic:

        - classes with ≤5 methods: documented inline
        - classes with >5 methods: separate pages for each method

        Parameters
        ----------
        package_name
            The name of the package.

        Returns
        -------
        list | None
            List of section dictionaries, or None if no sections could be created.
        """
        exports = self._get_package_exports(package_name)
        if not exports:
            return None

        # Filter out metadata variables at the export level too
        skip_names = {"__version__", "__author__", "__email__", "__all__"}
        exports = [e for e in exports if e not in skip_names]

        if not exports:
            return None

        print(f"Found {len(exports)} exported names to document")

        # Categorize the exports
        categories = self._categorize_api_objects(package_name, exports)

        sections = []

        # Use static threshold of 5 methods for large class separation
        method_threshold = 5

        # Add classes section if there are any
        if categories["classes"]:
            class_contents = []
            classes_with_separate_methods = []

            for class_name in categories["classes"]:
                method_count = categories["class_methods"].get(class_name, 0)

                if method_count > method_threshold:
                    # Class with many methods: add with members: [] to suppress inline docs
                    class_contents.append({"name": class_name, "members": []})
                    classes_with_separate_methods.append(class_name)
                else:
                    # Class with few methods: document inline
                    class_contents.append(class_name)

            sections.append(
                {
                    "title": "Classes",
                    "desc": "Core classes and types",
                    "contents": class_contents,
                }
            )

            # Create separate sections for methods of large classes
            for class_name in classes_with_separate_methods:
                method_names = categories["class_method_names"].get(class_name, [])
                method_count = len(method_names)

                # Create fully qualified method references
                method_contents = [f"{class_name}.{method}" for method in method_names]

                sections.append(
                    {
                        "title": f"{class_name} Methods",
                        "desc": f"Methods for the {class_name} class",
                        "contents": method_contents,
                    }
                )

                print(f"  Created separate section for {class_name} with {method_count} methods")

        # Add functions section if there are any
        if categories["functions"]:
            sections.append(
                {
                    "title": "Functions",
                    "desc": "Public functions",
                    "contents": categories["functions"],
                }
            )

        # Add other exports section if there are any
        if categories["other"]:
            sections.append(
                {"title": "Other", "desc": "Additional exports", "contents": categories["other"]}
            )

        return sections if sections else None

    def _extract_all_directives(self, package_name: str) -> dict:
        """
        Extract Great Docs directives from all docstrings in the package.

        Scans all exported classes, methods, and functions for @family, @order,
        @seealso, and @nodoc directives.

        Parameters
        ----------
        package_name
            The name of the package to scan.

        Returns
        -------
        dict
            Mapping of object names to their DocDirectives.
            Keys are either simple names (e.g., "MyClass") or qualified names
            (e.g., "MyClass.my_method").
        """
        from ._directives import extract_directives

        try:
            import griffe

            normalized_name = package_name.replace("-", "_")

            try:
                pkg = griffe.load(normalized_name)
            except Exception as e:
                print(f"Warning: Could not load package with griffe ({type(e).__name__})")
                return {}

            directive_map = {}

            # Use list() to materialize the iterator and catch any alias resolution errors
            try:
                members_list = list(pkg.members.items())
            except (griffe.CyclicAliasError, griffe.AliasResolutionError):
                # Some members have unresolvable aliases so try to iterate more carefully
                members_list = []
                for name in list(pkg.members.keys()):
                    try:
                        members_list.append((name, pkg.members[name]))
                    except Exception:
                        # Skip members that can't be accessed
                        continue
            except Exception:
                # Fall back to empty if we can't enumerate members at all
                return {}

            for name, obj in members_list:
                # Skip private members
                if name.startswith("_"):
                    continue

                # Skip aliases that can't be resolved (e.g., re-exports from external packages)
                try:
                    # Access kind to trigger alias resolution
                    _ = obj.kind
                except Exception:
                    # Silently skip unresolvable aliases since they're usually re-exports
                    # from external packages that wouldn't be documented anyway
                    continue

                # Extract directives from the object's docstring
                try:
                    if obj.docstring:
                        directives = extract_directives(obj.docstring.value)
                        if directives:
                            directive_map[name] = directives
                except Exception:
                    continue

                # For classes, also process methods
                try:
                    if obj.kind.value == "class":
                        for method_name, method in obj.members.items():
                            if method_name.startswith("_"):
                                continue
                            try:
                                if method.docstring:
                                    method_directives = extract_directives(method.docstring.value)
                                    if method_directives:
                                        directive_map[f"{name}.{method_name}"] = method_directives
                            except Exception:
                                continue
                except Exception:
                    # Skip if we can't introspect the class
                    pass

            return directive_map

        except ImportError:
            print("Warning: griffe not available for directive extraction")
            return {}

    def _build_sections_from_reference_config(
        self, reference_config: list[dict]
    ) -> list[dict] | None:
        """
        Build quartodoc sections directly from reference config without validation.

        This is a fallback method used when auto-discovery fails but the user has
        explicitly specified their API structure in great-docs.yml. Unlike
        `_create_quartodoc_sections_from_config`, this method doesn't attempt to
        validate the references against discovered exports.

        Parameters
        ----------
        reference_config
            The reference configuration from great-docs.yml.

        Returns
        -------
        list[dict] | None
            List of section dictionaries, or None if config is empty.
        """
        if not reference_config:
            return None

        sections = []

        for section_config in reference_config:
            title = section_config.get("title", "Untitled")
            desc = section_config.get("desc", "")
            contents_config = section_config.get("contents", [])

            if not contents_config:
                continue

            section_contents = []

            for item in contents_config:
                if isinstance(item, str):
                    # Simple string reference - use as-is
                    section_contents.append(item)
                elif isinstance(item, dict):
                    # Dict with name and optional members config
                    name = item.get("name", "")
                    if not name:
                        continue

                    members = item.get("members", True)

                    if members is False:
                        # Don't document methods - just the class
                        section_contents.append({"name": name, "members": []})
                    else:
                        # Default: inline documentation (members: true)
                        section_contents.append(name)

            if section_contents:
                sections.append(
                    {
                        "title": title,
                        "desc": desc,
                        "contents": section_contents,
                    }
                )

        return sections if sections else None

    def _create_quartodoc_sections_from_config(self, package_name: str) -> list | None:
        """
        Create quartodoc sections from the `reference` config in great-docs.yml.

        This method reads the explicit section configuration from great-docs.yml
        and generates quartodoc sections accordingly. If no `reference` config
        is provided, returns None to fall back to auto-discovery.

        Parameters
        ----------
        package_name
            The name of the package.

        Returns
        -------
        list | None
            List of section dictionaries, or None if no reference config.
        """
        reference_config = self._config.reference
        if not reference_config:
            return None

        print("Using explicit reference configuration from great-docs.yml")

        # Get all exports to validate references and get method info
        exports = self._get_package_exports(package_name)
        if not exports:
            print("Warning: Could not discover package exports for validation")
            exports = []

        # Categorize exports to get class method info
        categories = self._categorize_api_objects(package_name, exports)

        sections = []

        for section_config in reference_config:
            title = section_config.get("title", "Untitled")
            desc = section_config.get("desc", "")
            contents_config = section_config.get("contents", [])

            if not contents_config:
                continue

            section_contents = []

            for item in contents_config:
                if isinstance(item, str):
                    # Simple string reference - use as-is
                    section_contents.append(item)
                elif isinstance(item, dict):
                    # Dict with name and optional members config
                    name = item.get("name", "")
                    if not name:
                        continue

                    members = item.get("members", True)

                    if members is False:
                        # Don't document methods - just the class
                        section_contents.append({"name": name, "members": []})
                    else:
                        # Default: inline documentation (members: true)
                        section_contents.append(name)

            if section_contents:
                sections.append(
                    {
                        "title": title,
                        "desc": desc,
                        "contents": section_contents,
                    }
                )

        if sections:
            print(f"Generated {len(sections)} section(s) from reference config")

        return sections if sections else None

    def _create_quartodoc_sections_from_families(self, package_name: str) -> list | None:
        """
        Create quartodoc sections, prioritizing explicit config over auto-discovery.

        First checks for explicit `reference` configuration in great-docs.yml.
        If not found, falls back to auto-generating sections from discovered exports.

        Parameters
        ----------
        package_name
            The name of the package.

        Returns
        -------
        list | None
            List of section dictionaries organized by family, or None if
            no exports found.
        """
        # First, check for explicit reference config in great-docs.yml
        config_sections = self._create_quartodoc_sections_from_config(package_name)
        if config_sections:
            return config_sections

        # Fall back to auto-generated sections from discovered exports
        print("No reference config found, using auto-discovery")
        return self._create_quartodoc_sections(package_name)

    def _extract_authors_from_pyproject(self) -> list[dict[str, str]]:
        """
        Extract author information from pyproject.toml.

        Reads authors and maintainers from pyproject.toml and combines them
        into a list suitable for the great-docs.yml authors section.

        Returns
        -------
        list[dict[str, str]]
            List of author dictionaries with name, email, and role fields.
        """
        package_root = self._find_package_root()
        pyproject_path = package_root / "pyproject.toml"

        if not pyproject_path.exists():
            return []

        try:
            import tomllib

            with open(pyproject_path, "rb") as f:
                data = tomllib.load(f)
                project = data.get("project", {})

            authors_list = project.get("authors", [])
            maintainers_list = project.get("maintainers", [])

            # Track seen names to avoid duplicates
            seen_names: set[str] = set()
            result: list[dict[str, str]] = []

            # Process maintainers first (they get "Maintainer" role)
            for maintainer in maintainers_list:
                if isinstance(maintainer, dict):
                    name = maintainer.get("name", "")
                    email = maintainer.get("email", "")
                    if name and name not in seen_names:
                        seen_names.add(name)
                        author_entry: dict[str, str] = {"name": name, "role": "Maintainer"}
                        if email:
                            author_entry["email"] = email
                        result.append(author_entry)

            # Process authors (they get "Author" role, unless already added as maintainer)
            for author in authors_list:
                if isinstance(author, dict):
                    name = author.get("name", "")
                    email = author.get("email", "")
                    if name and name not in seen_names:
                        seen_names.add(name)
                        author_entry = {"name": name, "role": "Author"}
                        if email:
                            author_entry["email"] = email
                        result.append(author_entry)

            return result

        except Exception:
            return []

    def _format_authors_yaml(self, authors: list[dict[str, str]]) -> str:
        """
        Format authors list as YAML for great-docs.yml.

        Parameters
        ----------
        authors
            List of author dictionaries from _extract_authors_from_pyproject.

        Returns
        -------
        str
            YAML-formatted authors section, or empty string if no authors.
        """
        if not authors:
            return ""

        lines = [
            "# Author Information",
            "# ------------------",
            "# Author metadata for display in the landing page sidebar",
            "# You can add additional fields: github, orcid, affiliation, homepage",
            "authors:",
        ]

        for author in authors:
            lines.append(f"  - name: {author['name']}")
            if "role" in author:
                lines.append(f"    role: {author['role']}")
            # Add commented placeholders for optional fields
            lines.append("    # affiliation: ")
            if "email" in author:
                lines.append(f"    email: {author['email']}")
            else:
                lines.append("    # email: ")
            lines.append("    # github: ")
            lines.append("    # orcid: ")

        return "\n".join(lines)

    def _generate_initial_config(self, force: bool = False) -> bool:
        """
        Generate an initial great-docs.yml with discovered exports.

        Creates a great-docs.yml file in the project root with sensible defaults
        and a reference section populated from discovered package exports.

        Parameters
        ----------
        force
            If `True`, overwrite existing great-docs.yml without prompting.

        Returns
        -------
        bool
            True if config was created, False if skipped.
        """
        config_path = self._find_package_root() / "great-docs.yml"

        # Check if config already exists
        if config_path.exists() and not force:
            response = input(f"{config_path} already exists. Overwrite? [y/N]: ")
            if response.lower() != "y":
                print("Skipping great-docs.yml")
                return False

        # Detect package name
        package_name = self._detect_package_name()
        if not package_name:
            print("Warning: Could not detect package name, creating minimal config")
            # Create minimal config without reference section
            config_content = self._generate_minimal_config()
            config_path.write_text(config_content, encoding="utf-8")
            print(f"Created {config_path}")
            return True

        # Get normalized package name for imports
        importable_name = self._normalize_package_name(package_name)

        # Detect docstring style
        print("Detecting docstring style...")
        parser_style = self._detect_docstring_style(importable_name)

        # Discover exports
        exports = self._get_package_exports(importable_name)
        if not exports:
            print("Warning: Could not discover exports, creating minimal config")
            # Without exports, we still try to detect dynamic mode
            print("Testing dynamic introspection mode...")
            dynamic_mode = self._detect_dynamic_mode(importable_name)
            config_content = self._generate_minimal_config(
                parser=parser_style, dynamic=dynamic_mode
            )
            config_path.write_text(config_content, encoding="utf-8")
            print(f"Created {config_path}")
            return True

        # Categorize exports (this also detects cyclic aliases)
        print("Categorizing API objects...")
        categories = self._categorize_api_objects(importable_name, exports)

        # Determine dynamic mode based on cyclic alias detection during categorization
        cyclic_alias_count = categories.get("cyclic_alias_count", 0)
        if cyclic_alias_count > 0:
            print(f"Detected {cyclic_alias_count} cyclic alias(es), using dynamic: false")
            dynamic_mode = False
        else:
            # Run the explicit detection as a fallback
            print("Testing dynamic introspection mode...")
            dynamic_mode = self._detect_dynamic_mode(importable_name)

        # Generate config content
        config_content = self._generate_config_with_reference(
            categories, importable_name, parser=parser_style, dynamic=dynamic_mode
        )

        config_path.write_text(config_content, encoding="utf-8")
        print(f"Created {config_path}")
        return True

    def _generate_minimal_config(self, parser: str = "numpy", dynamic: bool = True) -> str:
        """
        Generate minimal great-docs.yml without reference section.

        Parameters
        ----------
        parser
            The docstring parser style ("numpy", "google", or "sphinx").
        dynamic
            Whether to use dynamic introspection mode for quartodoc.

        Returns
        -------
        str
            YAML content for a minimal configuration file.
        """
        dynamic_str = "true" if dynamic else "false"

        # Extract authors from pyproject.toml
        authors = self._extract_authors_from_pyproject()
        authors_yaml = self._format_authors_yaml(authors)

        # Build the config with optional authors section
        authors_section = f"\n{authors_yaml}\n" if authors_yaml else ""

        return f"""# Great Docs Configuration
# See https://rich-iannone.github.io/great-docs/user-guide/03-configuration.html

# Module Name (optional)
# ----------------------
# Set this if your importable module name differs from the project name.
# Example: project 'py-yaml12' with module name 'yaml12'
# module: yaml12

# Docstring Parser
# ----------------
# The docstring format used in your package (numpy, google, or sphinx)
parser: {parser}

# Dynamic Introspection
# ---------------------
# Use runtime introspection for more accurate documentation (default: true)
# Set to false if your package has cyclic alias issues (e.g., PyO3/Rust bindings)
dynamic: {dynamic_str}

# Exclusions
# ----------
# Items to exclude from auto-documentation (affects 'init' and 'scan')
# exclude:
#   - InternalClass
#   - helper_function
{authors_section}
# Site Settings
# -------------
# site:
#   theme: flatly              # Quarto theme (default: flatly)
#   toc: true                  # Show table of contents (default: true)
#   toc-depth: 2               # TOC heading depth (default: 2)
#   toc-title: On this page    # TOC title (default: "On this page")

# Jupyter Kernel
# --------------
# Jupyter kernel to use for executing code cells in .qmd files.
# This is set at the project level so it applies to all pages, including
# auto-generated API reference pages. Can be overridden in individual .qmd
# file frontmatter if needed for special cases.
# jupyter: python3             # Default: python3

# API Reference Structure
# -----------------------
# Auto-discovery couldn't determine your package's public API.
# You can manually specify which items to document here.
#
# Uncomment and customize the reference section below:
#
# reference:
#   - title: Functions
#     desc: Public functions provided by the package
#     contents:
#       - my_function
#       - another_function
#
#   - title: Classes
#     desc: Main classes for working with the package
#     contents:
#       - name: MyClass
#         members: false       # Don't document methods inline
#       - SimpleClass          # Methods documented inline (default)
#
# After editing, run 'great-docs build' to generate your documentation.
"""

    def _generate_config_with_reference(
        self, categories: dict, package_name: str, parser: str = "numpy", dynamic: bool = True
    ) -> str:
        """
        Generate great-docs.yml with a reference section from discovered exports.

        Parameters
        ----------
        categories
            Dictionary from _categorize_api_objects with classes, functions, other.
        package_name
            The package name (for method threshold comments).
        parser
            The docstring parser style ("numpy", "google", or "sphinx").
        dynamic
            Whether to use dynamic introspection mode for quartodoc.

        Returns
        -------
        str
            YAML content for the configuration file.
        """
        dynamic_str = "true" if dynamic else "false"

        # Extract authors from pyproject.toml
        authors = self._extract_authors_from_pyproject()
        authors_yaml = self._format_authors_yaml(authors)

        lines = [
            "# Great Docs Configuration",
            "# See https://rich-iannone.github.io/great-docs/user-guide/03-configuration.html",
            "",
            "# Module Name (optional)",
            "# ----------------------",
            "# Set this if your importable module name differs from the project name.",
            "# Example: project 'py-yaml12' with module name 'yaml12'",
            "# module: yaml12",
            "",
            "# Docstring Parser",
            "# ----------------",
            "# The docstring format used in your package (numpy, google, or sphinx)",
            f"parser: {parser}",
            "",
            "# Dynamic Introspection",
            "# ---------------------",
            "# Use runtime introspection for more accurate documentation (default: true)",
            "# Set to false if your package has cyclic alias issues (e.g., PyO3/Rust bindings)",
            f"dynamic: {dynamic_str}",
            "",
            "# API Discovery Settings",
            "# ----------------------",
            "# Exclude items from auto-documentation",
            "# exclude:",
            "#   - InternalClass",
            "#   - helper_function",
            "",
        ]

        # Add authors section if we found any
        if authors_yaml:
            lines.append(authors_yaml)
            lines.append("")

        # Add reference section
        lines.extend(
            [
                "# API Reference Structure",
                "# -----------------------",
                "# Customize the sections below to organize your API documentation.",
                "# - Reorder items within a section to change their display order",
                "# - Move items between sections or create new sections",
                "# - Use 'members: false' to exclude methods from documentation",
                "# - Add 'desc:' to sections for descriptions",
                "",
                "reference:",
            ]
        )

        classes = categories.get("classes", [])
        functions = categories.get("functions", [])
        other = categories.get("other", [])
        class_methods = categories.get("class_methods", {})
        class_method_names = categories.get("class_method_names", {})

        # Use static threshold of 5 methods for large class separation
        threshold = 5

        # Track large classes that need separate method sections
        large_classes = []

        # Classes section
        if classes:
            lines.append("  - title: Classes")
            lines.append("    desc: Main classes provided by the package")
            lines.append("    contents:")
            for class_name in sorted(classes):
                method_count = class_methods.get(class_name, 0)
                if method_count > threshold:
                    # Large class: use members: false, methods listed separately
                    lines.append(f"      - name: {class_name}")
                    lines.append(f"        members: false  # {method_count} methods listed below")
                    large_classes.append(class_name)
                elif method_count > 0:
                    lines.append(f"      - {class_name}  # {method_count} method(s)")
                else:
                    lines.append(f"      - {class_name}")

        # Add separate method sections for large classes
        for class_name in large_classes:
            method_names = class_method_names.get(class_name, [])
            if method_names:
                lines.append("")
                lines.append(f"  - title: {class_name} Methods")
                lines.append(f"    desc: Methods for the {class_name} class")
                lines.append("    contents:")
                for method_name in method_names:
                    lines.append(f"      - {class_name}.{method_name}")

        # Functions section
        if functions:
            if classes or large_classes:
                lines.append("")
            lines.append("  - title: Functions")
            lines.append("    desc: Utility functions")
            lines.append("    contents:")
            for func_name in sorted(functions):
                lines.append(f"      - {func_name}")

        # Other section (if any non-class, non-function exports)
        if other:
            if classes or functions:
                lines.append("")
            lines.append("  - title: Other")
            lines.append("    desc: Additional exports")
            lines.append("    contents:")
            for other_name in sorted(other):
                lines.append(f"      - {other_name}")

        # Add trailing sections for site settings (commented out)
        lines.extend(
            [
                "",
                "# Site Settings",
                "# -------------",
                "# site:",
                "#   theme: flatly              # Quarto theme (default: flatly)",
                "#   toc: true                  # Show table of contents (default: true)",
                "#   toc-depth: 2               # TOC heading depth (default: 2)",
                "#   toc-title: On this page    # TOC title",
                "",
                "# Jupyter Kernel",
                "# --------------",
                "# Jupyter kernel to use for executing code cells in .qmd files.",
                "# This is set at the project level so it applies to all pages, including",
                "# auto-generated API reference pages. Can be overridden in individual .qmd",
                "# file frontmatter if needed for special cases.",
                "# jupyter: python3             # Default: python3",
                "",
                "# CLI Documentation",
                "# -----------------",
                "# cli:",
                "#   enabled: true              # Enable CLI documentation",
                "#   module: my_package.cli     # Module containing Click commands",
                "#   name: cli                  # Name of the Click command object",
            ]
        )

        return "\n".join(lines) + "\n"

    def _find_index_source_file(self) -> tuple[Path | None, list[str]]:
        """
        Find the best source file for index.qmd based on priority.

        Priority order (highest to lowest):
        1. index.qmd in project root
        2. index.md in project root
        3. README.md in project root

        Returns
        -------
        tuple[Path | None, list[str]]
            A tuple of (source_file_path, warnings_list).
            source_file_path is None if no suitable file is found.
        """
        package_root = self._find_package_root()
        warnings = []

        index_qmd_root = package_root / "index.qmd"
        index_md_root = package_root / "index.md"
        readme_path = package_root / "README.md"

        # Check which files exist
        has_index_qmd = index_qmd_root.exists()
        has_index_md = index_md_root.exists()
        has_readme = readme_path.exists()

        # Generate warnings for multiple source files
        if has_index_qmd and (has_index_md or has_readme):
            other_files = []
            if has_index_md:
                other_files.append("index.md")
            if has_readme:
                other_files.append("README.md")
            warnings.append(
                f"⚠️  Multiple index source files detected. Using index.qmd "
                f"(ignoring {', '.join(other_files)})"
            )
            return index_qmd_root, warnings

        if has_index_md and has_readme:
            warnings.append(
                "⚠️  Multiple index source files detected. Using index.md (ignoring README.md)"
            )
            return index_md_root, warnings

        # Return based on priority
        if has_index_qmd:
            return index_qmd_root, warnings
        if has_index_md:
            return index_md_root, warnings
        if has_readme:
            return readme_path, warnings

        return None, warnings

    def _create_index_from_readme(self, force_rebuild: bool = False) -> None:
        """
        Create or update index.qmd from the best available source file.

        Source file priority (highest to lowest):
        1. index.qmd in project root
        2. index.md in project root
        3. README.md in project root

        This mimics pkgdown's behavior of using the README as the homepage.
        Includes a metadata sidebar with package information (license, authors, links, etc.)

        Parameters
        ----------
        force_rebuild
            If `True`, always rebuild `index.qmd` even if it exists. Used by the build command to
            sync with source file changes.
        """
        package_root = self._find_package_root()

        # Always create license.qmd if LICENSE file exists
        license_path = package_root / "LICENSE"
        license_link = None
        if license_path.exists():
            license_qmd = self.project_path / "license.qmd"
            with open(license_path, "r", encoding="utf-8") as f:
                license_content = f.read()

            license_qmd_content = f"""---
title: "License"
---

```
{license_content}
```
"""
            with open(license_qmd, "w", encoding="utf-8") as f:
                f.write(license_qmd_content)
            print(f"Created {license_qmd}")
            license_link = "license.qmd"

        # Always create citation.qmd if CITATION.cff exists
        citation_path = package_root / "CITATION.cff"
        citation_link = None
        if citation_path.exists():
            citation_qmd = self.project_path / "citation.qmd"

            # Get metadata first to access rich_authors
            metadata = self._get_package_metadata()

            # Parse CITATION.cff for structured data
            import yaml

            with open(citation_path, "r", encoding="utf-8") as f:
                citation_data = yaml.safe_load(f)

            # Build Authors section
            authors_section = "## Authors\n\n"
            if citation_data.get("authors"):
                for author in citation_data["authors"]:
                    given = author.get("given-names", "")
                    family = author.get("family-names", "")
                    full_name = f"{given} {family}".strip()

                    # Get role from rich_authors if available
                    role = "Author"
                    if metadata.get("rich_authors"):
                        for rich_author in metadata["rich_authors"]:
                            if rich_author.get("name") == full_name:
                                role = rich_author.get("role", "Author")
                                break

                    authors_section += f"{full_name}. {role}.  \n"

            # Build Citation section with text and BibTeX
            citation_section = "## Citation\n\n"
            citation_section += "**Source:** `CITATION.cff`\n\n"

            # Generate text citation
            if citation_data.get("authors"):
                author_names = []
                for author in citation_data["authors"]:
                    family = author.get("family-names", "")
                    given = author.get("given-names", "")
                    initial = given[0] if given else ""
                    author_names.append(f"{family} {initial}" if initial else family)

                authors_str = ", ".join(author_names)
                title = citation_data.get("title", "")
                version = citation_data.get("version", "")
                url = citation_data.get("url", "")

                # Parse year from date-released field or use current year
                from datetime import datetime

                year = datetime.now().year
                if citation_data.get("date-released"):
                    try:
                        date_released = citation_data["date-released"]
                        if isinstance(date_released, str):
                            year = datetime.fromisoformat(date_released).year
                        elif hasattr(date_released, "year"):
                            year = date_released.year
                    except (ValueError, AttributeError):
                        pass  # Keep current year as fallback

                citation_section += (
                    f"{authors_str} ({year}). {title} Python package version {version}, {url}.\n\n"
                )

            # Generate BibTeX
            citation_section += "```bibtex\n"
            citation_section += "@Manual{,\n"

            if citation_data.get("title"):
                citation_section += f"  title = {{{citation_data['title']}}},\n"

            if citation_data.get("authors"):
                author_names = []
                for author in citation_data["authors"]:
                    given = author.get("given-names", "")
                    family = author.get("family-names", "")
                    full_name = f"{given} {family}".strip()
                    author_names.append(full_name)
                citation_section += f"  author = {{{' and '.join(author_names)}}},\n"

            # Parse year from date-released or use current year
            from datetime import datetime

            year = datetime.now().year
            if citation_data.get("date-released"):
                try:
                    date_released = citation_data["date-released"]
                    if isinstance(date_released, str):
                        year = datetime.fromisoformat(date_released).year
                    elif hasattr(date_released, "year"):
                        year = date_released.year
                except (ValueError, AttributeError):
                    pass  # Keep current year as fallback

            citation_section += f"  year = {{{year}}},\n"

            if citation_data.get("version"):
                citation_section += (
                    f"  note = {{Python package version {citation_data['version']}}},\n"
                )

            if citation_data.get("url"):
                citation_section += f"  url = {{{citation_data['url']}}},\n"

            citation_section += "}\n```\n"

            citation_qmd_content = f"""---
title: "Authors and Citation"
---

{authors_section}

{citation_section}
"""
            with open(citation_qmd, "w", encoding="utf-8") as f:
                f.write(citation_qmd_content)
            print(f"Created {citation_qmd}")
            citation_link = "citation.qmd"

        # Now check if we should create index.qmd
        index_qmd = self.project_path / "index.qmd"

        if index_qmd.exists() and not force_rebuild:
            print("index.qmd already exists, skipping creation")
            return

        # Find the best source file
        source_file, warnings = self._find_index_source_file()

        # Print any warnings about multiple source files
        for warning in warnings:
            print(warning)

        if source_file is None:
            print(
                "No index source file found (index.qmd, index.md, or README.md), skipping index.qmd creation"
            )
            return

        source_name = source_file.name
        if force_rebuild:
            print(f"Rebuilding index.qmd from {source_name}...")
        else:
            print(f"Creating index.qmd from {source_name}...")

        # Read source content
        with open(source_file, "r", encoding="utf-8") as f:
            readme_content = f.read()

        # Adjust heading levels: bump all headings up by one level
        # This prevents h1 from becoming paragraphs and keeps proper hierarchy
        # Replace headings from highest to lowest level to avoid double-replacement
        import re

        readme_content = re.sub(r"^######\s+", r"####### ", readme_content, flags=re.MULTILINE)
        readme_content = re.sub(r"^#####\s+", r"###### ", readme_content, flags=re.MULTILINE)
        readme_content = re.sub(r"^####\s+", r"##### ", readme_content, flags=re.MULTILINE)
        readme_content = re.sub(r"^###\s+", r"#### ", readme_content, flags=re.MULTILINE)
        readme_content = re.sub(r"^##\s+", r"### ", readme_content, flags=re.MULTILINE)
        readme_content = re.sub(r"^#\s+", r"## ", readme_content, flags=re.MULTILINE)

        # Get package metadata for sidebar
        metadata = self._get_package_metadata()

        # Build margin content sections (right sidebar)
        margin_sections = []

        # Links section
        links_added = []

        # Try to add PyPI link based on package name
        package_name = self._detect_package_name()
        if package_name:
            pypi_url = f"https://pypi.org/project/{package_name}/"
            margin_sections.append("#### Links\n")
            margin_sections.append(f"[View on PyPI]({pypi_url})<br>")
            links_added.append("pypi")

        if metadata.get("urls"):
            if not links_added:
                margin_sections.append("#### Links\n")

            urls = metadata["urls"]

            # Map common URL names to display text
            url_map = {
                "homepage": None,  # Skip if we already added PyPI
                "repository": "Browse source code",
                "bug_tracker": "Report a bug",
                "documentation": None,  # Skip for that's the site we're on
            }

            for name, url in urls.items():
                name_lower = name.lower().replace(" ", "_")
                display_name = url_map.get(name_lower, name.replace("_", " ").title())

                # Skip if display_name is None (homepage/documentation)
                if display_name:
                    margin_sections.append(f"[{display_name}]({url})<br>")

        # Add llms.txt and llms-full.txt links at the end of Links section
        if links_added or metadata.get("urls"):
            margin_sections.append("[llms.txt](llms.txt)<br>")
            margin_sections.append("[llms-full.txt](llms-full.txt)<br>")
        else:
            # If no links section exists yet, create one just for llms.txt files
            margin_sections.append("#### Links\n")
            margin_sections.append("[llms.txt](llms.txt)<br>")
            margin_sections.append("[llms-full.txt](llms-full.txt)<br>")

        # License section
        if license_link:
            margin_sections.append("\n#### License\n")
            margin_sections.append(f"[Full license]({license_link})<br>")
        elif metadata.get("license"):
            margin_sections.append("\n#### License\n")
            margin_sections.append(f"{metadata['license']}")

        # Community section: check for CONTRIBUTING.md and CODE_OF_CONDUCT.md
        community_items = []

        # Check for CONTRIBUTING.md in root or .github directory
        contributing_path = package_root / "CONTRIBUTING.md"
        if not contributing_path.exists():
            contributing_path = package_root / ".github" / "CONTRIBUTING.md"

        # Check for CODE_OF_CONDUCT.md in root or .github directory
        coc_path = package_root / "CODE_OF_CONDUCT.md"
        if not coc_path.exists():
            coc_path = package_root / ".github" / "CODE_OF_CONDUCT.md"

        if contributing_path.exists():
            community_items.append("[Contributing guide](contributing.qmd)<br>")
            # Create contributing.qmd
            with open(contributing_path, "r", encoding="utf-8") as f:
                contributing_content = f.read()

            # Strip first heading if it exists to avoid duplication with title
            lines = contributing_content.split("\n")
            if lines and lines[0].startswith("# "):
                contributing_content = "\n".join(lines[1:]).lstrip()

            contributing_qmd = self.project_path / "contributing.qmd"
            contributing_qmd_content = f"""---
title: "Contributing"
---

{contributing_content}
"""
            with open(contributing_qmd, "w", encoding="utf-8") as f:
                f.write(contributing_qmd_content)
            print(f"Created {contributing_qmd}")

        if coc_path.exists():
            community_items.append("[Code of conduct](code-of-conduct.qmd)<br>")
            # Create code-of-conduct.qmd
            with open(coc_path, "r", encoding="utf-8") as f:
                coc_content = f.read()

            # Strip first heading if it exists to avoid duplication with title
            lines = coc_content.split("\n")
            if lines and lines[0].startswith("# "):
                coc_content = "\n".join(lines[1:]).lstrip()

            coc_qmd = self.project_path / "code-of-conduct.qmd"
            coc_qmd_content = f"""---
title: "Code of Conduct"
---

{coc_content}
"""
            with open(coc_qmd, "w", encoding="utf-8") as f:
                f.write(coc_qmd_content)
            print(f"Created {coc_qmd}")

        if community_items:
            margin_sections.append("\n#### Community\n")
            margin_sections.extend(community_items)

        # Developers section (Authors)
        # Use rich author metadata if available, otherwise fall back to standard authors
        authors_to_display = metadata.get("rich_authors") or metadata.get("authors", [])

        if authors_to_display:
            margin_sections.append("\n#### Developers\n")

            # Try to extract GitHub username from repository URL as fallback
            fallback_github = None
            if metadata.get("urls"):
                repo_url = metadata["urls"].get("repository", "") or metadata["urls"].get(
                    "Repository", ""
                )
                if "github.com/" in repo_url:
                    # Extract username from URL like https://github.com/username/repo
                    parts = repo_url.rstrip("/").split("github.com/")
                    if len(parts) > 1:
                        username_part = parts[1].split("/")[0]
                        if username_part:
                            fallback_github = username_part

            for idx, author in enumerate(authors_to_display):
                if isinstance(author, dict):
                    name = author.get("name", "")
                    email = author.get("email", "")

                    # Rich metadata fields (from great-docs.yml authors)
                    role = author.get("role", "")
                    affiliation = author.get("affiliation", "")
                    github = author.get("github", "")
                    homepage = author.get("homepage", "")
                    orcid = author.get("orcid", "")

                    # Build author line with name
                    author_parts = [f"**{name}**" if role else name]

                    # Add role/affiliation on separate lines if available
                    if role:
                        author_parts.append(f"<br><small>{role}</small>")
                    if affiliation:
                        author_parts.append(
                            f'<br><small style="margin-top: -0.15em; display: block;">{affiliation}</small>'
                        )

                    # Add icon links
                    icon_links = []

                    if email:
                        icon_links.append(
                            f'<a href="mailto:{email}" title="Email"><i class="bi bi-envelope-fill"></i></a>'
                        )

                    if github:
                        icon_links.append(
                            f'<a href="https://github.com/{github}" title="GitHub"><i class="bi bi-github"></i></a>'
                        )
                    elif fallback_github:
                        icon_links.append(
                            f'<a href="https://github.com/{fallback_github}" title="GitHub"><i class="bi bi-github"></i></a>'
                        )

                    if homepage:
                        icon_links.append(
                            f'<a href="{homepage}" title="Homepage"><i class="bi bi-house-fill"></i></a>'
                        )

                    if orcid:
                        # ORCID should be a full URL or just the ID
                        orcid_url = (
                            orcid if orcid.startswith("http") else f"https://orcid.org/{orcid}"
                        )
                        icon_links.append(
                            f'<a href="{orcid_url}" title="ORCID"><i class="fa-brands fa-orcid"></i></a>'
                        )

                    if icon_links:
                        author_parts.append(
                            '<span style="margin-top: -0.15em; display: block;">'
                            + " ".join(icon_links)
                            + "</span>"
                        )

                    # Wrap in <p> tag with padding for non-first authors
                    author_content = " ".join(author_parts)
                    if idx == 0:
                        margin_sections.append(f"<p>{author_content}</p>")
                    else:
                        margin_sections.append(
                            f'<p style="padding-top: 10px;">{author_content}</p>'
                        )

        # Meta section (Python version and extras)
        meta_items = []
        if metadata.get("requires_python"):
            meta_items.append(f"**Requires:** Python `{metadata['requires_python']}`")

        if metadata.get("optional_dependencies"):
            extras = list(metadata["optional_dependencies"].keys())
            if extras:
                # Wrap each extra in backticks for monospace
                extras_formatted = ", ".join(f"`{extra}`" for extra in extras)
                meta_items.append(f"**Provides-Extra:** {extras_formatted}")

        if meta_items:
            margin_sections.append("\n#### Meta\n")
            margin_sections.append("<br>\n".join(meta_items))

        # Citation section (if CITATION.cff exists)
        if citation_link:
            margin_sections.append("\n#### Citation\n")
            margin_sections.append(f"[Citing great-docs]({citation_link})")

        # Build margin content
        margin_content = "\n".join(margin_sections) if margin_sections else ""

        # CSS to reduce top margin of first heading element
        # The heading ends up inside a section.level1 > h1 structure
        first_heading_style = """<style>
section.level1:first-of-type > h1:first-child,
section.level2:first-of-type > h2:first-child,
.column-body-outset-right > section.level1:first-of-type > h1,
#quarto-document-content > section:first-of-type > h1 {
  margin-top: 4px !important;
}
</style>

"""

        # Create a qmd file with the README content
        # Use empty title so "Home" doesn't appear on landing page
        # Add margin content in a special div that Quarto will place in the margin
        if margin_content:
            qmd_content = f"""---
title: ""
toc: false
---

{first_heading_style}::: {{.column-margin}}
{margin_content}
:::

{readme_content}
"""
        else:
            qmd_content = f"""---
title: ""
toc: false
---

{first_heading_style}{readme_content}
"""

        with open(index_qmd, "w", encoding="utf-8") as f:
            f.write(qmd_content)

        print(f"Created {index_qmd}")

    def _add_quartodoc_config(self) -> None:
        """
        Add quartodoc configuration to _quarto.yml if not present.

        Adds sensible defaults for quartodoc with automatic package detection.
        """
        quarto_yml = self.project_path / "_quarto.yml"

        with open(quarto_yml, "r") as f:
            config = yaml.safe_load(f) or {}

        # Check if quartodoc config already exists
        if "quartodoc" in config:
            print("quartodoc configuration already exists, skipping")
            return

        # Detect package name (project name from pyproject.toml)
        package_name = self._detect_package_name()

        if not package_name:
            response = input(
                "\nCould not auto-detect package name. Enter package name for quartodoc (or press Enter to skip): "
            ).strip()
            if not response:
                print("Skipping quartodoc configuration")
                return
            package_name = response

        print(f"Adding quartodoc configuration for package: {package_name}")

        # Detect actual module name (may differ from project name)
        # e.g., project 'py-yaml12' might have module 'yaml12'
        module_name = self._detect_module_name()
        if module_name:
            importable_name = module_name
            if module_name != package_name and module_name != self._normalize_package_name(
                package_name
            ):
                print(f"Detected module name: {module_name}")
        else:
            # Fall back to normalizing package name (hyphens -> underscores)
            importable_name = self._normalize_package_name(package_name)

        # Check if this is a compiled extension
        if self._is_compiled_extension():
            print("Detected compiled extension package (PyO3/Rust/Cython)")
            print("Note: The package must be installed (`pip install -e .` or `maturin develop`)")
            print("      for quartodoc to generate documentation.")

        # Try to auto-generate sections from discovered exports
        # First try family-based organization (from @family directives)
        # Falls back to default categorization if no directives found
        sections = self._create_quartodoc_sections_from_families(importable_name)

        # Add quartodoc configuration with sensible defaults
        # Use the importable name (actual module name) for the package field
        quartodoc_config = {
            "package": importable_name,
            "dir": "reference",
            "title": "Reference",
            "style": "pkgdown",
            "renderer": {"style": "markdown", "table_style": "description-list"},
        }

        # Get dynamic setting from great-docs.yml config (defaults to True)
        dynamic = self._config.dynamic
        quartodoc_config["dynamic"] = dynamic
        if not dynamic:
            print("Using static introspection mode (dynamic: false)")

        # Get parser from great-docs.yml config (defaults to numpy)
        parser = self._config.parser
        if parser and parser != "numpy":
            # Only add parser if it's not the default (numpy)
            quartodoc_config["parser"] = parser
            print(f"Using '{parser}' docstring parser")
        else:
            # Always explicitly set parser for clarity
            quartodoc_config["parser"] = "numpy"

        # Get jupyter kernel from great-docs.yml config (defaults to python3)
        jupyter_kernel = self._config.jupyter
        if jupyter_kernel:
            config["jupyter"] = jupyter_kernel
            print(f"Using '{jupyter_kernel}' Jupyter kernel")

        # Add sections if we found them
        if sections:
            quartodoc_config["sections"] = sections
            print(f"Auto-generated {len(sections)} section(s) from package exports")
        else:
            print("Could not auto-generate sections from package exports")
            print("You'll need to manually add sections to organize your API documentation.")

        config["quartodoc"] = quartodoc_config

        # Write back to file
        self._write_quarto_yml(quarto_yml, config)

        print(f"Added quartodoc configuration to {quarto_yml}")
        if not sections:
            print("See: https://machow.github.io/quartodoc/get-started/overview.html")

    def _refresh_quartodoc_config(self) -> None:
        """
        Refresh the quartodoc sections in _quarto.yml based on current package exports.

        This method re-discovers the package API and updates the quartodoc sections without touching
        other configuration. Use this when your package API has changed (new classes, methods, or
        functions added/removed).

        The method preserves:

        - package name and other quartodoc settings
        - all non-quartodoc configuration in _quarto.yml

        Only the 'sections' key in quartodoc config is regenerated.
        """
        quarto_yml = self.project_path / "_quarto.yml"

        if not quarto_yml.exists():
            print("Error: _quarto.yml not found. Run 'great-docs init' first.")
            return

        with open(quarto_yml, "r") as f:
            config = yaml.safe_load(f) or {}

        if "quartodoc" not in config:
            print("Error: No quartodoc configuration found. Run 'great-docs init' first.")
            return

        # Get the package name from existing config
        package_name = config["quartodoc"].get("package")
        if not package_name:
            print("Error: No package name in quartodoc config.")
            return

        print(f"Re-discovering exports for package: {package_name}")

        # Re-generate sections from current package exports
        # Uses family-based organization if @family directives are found
        # Prioritizes explicit config from great-docs.yml over auto-discovery
        sections = self._create_quartodoc_sections_from_families(package_name)

        # Update parser and dynamic settings from great-docs.yml config
        parser = self._config.parser
        if parser:
            config["quartodoc"]["parser"] = parser

        dynamic = self._config.dynamic
        config["quartodoc"]["dynamic"] = dynamic

        # Update jupyter kernel from great-docs.yml config
        jupyter_kernel = self._config.jupyter
        if jupyter_kernel:
            config["jupyter"] = jupyter_kernel

        if sections:
            config["quartodoc"]["sections"] = sections
            print(f"Updated quartodoc config with {len(sections)} section(s)")

            # Write back to file first, so sidebar update reads the new sections
            self._write_quarto_yml(quarto_yml, config)

            # Now update the sidebar to match the new sections
            self._update_sidebar_from_sections()

            print(f"✅ Refreshed quartodoc configuration in {quarto_yml}")
        else:
            # Check if user has explicit reference config that should be applied
            # even though auto-discovery failed
            reference_config = self._config.reference
            if reference_config:
                print(
                    "Auto-discovery failed, but applying explicit reference config from great-docs.yml"
                )
                # Build sections directly from user config without validation
                user_sections = self._build_sections_from_reference_config(reference_config)
                if user_sections:
                    config["quartodoc"]["sections"] = user_sections
                    self._write_quarto_yml(quarto_yml, config)
                    self._update_sidebar_from_sections()
                    print(f"✅ Applied {len(user_sections)} section(s) from great-docs.yml")
                else:
                    print("Warning: reference config in great-docs.yml produced no sections")
            else:
                print("Warning: Could not discover package exports. Config unchanged.")
                print(
                    "Tip: Add a 'reference' section to great-docs.yml to manually specify your API structure."
                )

    def _write_quarto_yml(self, quarto_yml: Path, config: dict) -> None:
        """
        Write _quarto.yml with a header comment.

        Parameters
        ----------
        quarto_yml
            Path to the _quarto.yml file.
        config
            The configuration dictionary to write.
        """
        header_comment = (
            "# Generated by Great Docs - Do not modify this file by hand.\n"
            "# Configure settings in great-docs.yml instead.\n\n"
        )
        with open(quarto_yml, "w") as f:
            f.write(header_comment)
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)

    def _update_quarto_config(self) -> None:
        """
        Update _quarto.yml with great-docs configuration.

        This private method modifies the Quarto configuration file to include the
        post-render script, CSS file, and website navigation required by great-docs.
        It preserves existing configuration while adding the necessary great-docs
        settings. If website navigation is not present, it adds a navbar with Home
        and API Reference links, and sets the site title to the package name.
        """
        quarto_yml = self.project_path / "_quarto.yml"

        if not quarto_yml.exists():
            print("Warning: _quarto.yml not found. Creating minimal configuration...")
            config = {
                "project": {"type": "website", "post-render": "scripts/post-render.py"},
                "format": {"html": {"theme": "flatly", "css": ["great-docs.css"]}},
            }
        else:
            # Load existing configuration
            with open(quarto_yml, "r") as f:
                config = yaml.safe_load(f) or {}

        # Ensure required structure exists
        if "project" not in config:
            config["project"] = {}
        if "format" not in config:
            config["format"] = {}
        if "html" not in config["format"]:
            config["format"]["html"] = {}

        # Add post-render script
        config["project"]["post-render"] = "scripts/post-render.py"

        # Add resources to copy static JS files to _site
        if "resources" not in config["project"]:
            config["project"]["resources"] = []
        elif isinstance(config["project"]["resources"], str):
            config["project"]["resources"] = [config["project"]["resources"]]

        # Ensure JS files are included as resources
        for js_file in [
            "github-widget.js",
            "sidebar-filter.js",
            "dark-mode-toggle.js",
            "theme-init.js",
        ]:
            if js_file not in config["project"]["resources"]:
                config["project"]["resources"].append(js_file)

        # Add CSS file
        if "css" not in config["format"]["html"]:
            config["format"]["html"]["css"] = []
        elif isinstance(config["format"]["html"]["css"], str):
            config["format"]["html"]["css"] = [config["format"]["html"]["css"]]

        if "great-docs.css" not in config["format"]["html"]["css"]:
            config["format"]["html"]["css"].append("great-docs.css")

        # Apply site settings from great-docs.yml (forwarded to format.html)
        site_settings = self._config.site
        config["format"]["html"]["theme"] = site_settings.get("theme", "flatly")
        config["format"]["html"]["toc"] = site_settings.get("toc", True)
        config["format"]["html"]["toc-depth"] = site_settings.get("toc-depth", 2)
        config["format"]["html"]["toc-title"] = site_settings.get("toc-title", "On this page")

        if "shift-heading-level-by" not in config["format"]["html"]:
            config["format"]["html"]["shift-heading-level-by"] = -1

        # Add Font Awesome for ORCID icon support
        if "include-in-header" not in config["format"]["html"]:
            config["format"]["html"]["include-in-header"] = []
        elif isinstance(config["format"]["html"]["include-in-header"], str):
            config["format"]["html"]["include-in-header"] = [
                config["format"]["html"]["include-in-header"]
            ]

        # Add Font Awesome CDN if not already present
        fa_cdn = '<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">'
        fa_entry = {"text": fa_cdn}
        if fa_entry not in config["format"]["html"]["include-in-header"]:
            # Check if any Font Awesome link already exists
            has_fa = any(
                "font-awesome" in str(item).lower()
                for item in config["format"]["html"]["include-in-header"]
            )
            if not has_fa:
                config["format"]["html"]["include-in-header"].append(fa_entry)

        # Add website navigation if not present
        if "website" not in config:
            config["website"] = {}

        # Enable page navigation for TOC
        if "page-navigation" not in config["website"]:
            config["website"]["page-navigation"] = True

        # Set title to package name if not already set
        if "title" not in config["website"]:
            package_name = self._detect_package_name()
            if package_name:
                config["website"]["title"] = package_name.title()

        # Get GitHub info and style preference
        owner, repo, repo_url = self._get_github_repo_info()
        metadata = self._get_package_metadata()
        github_style = metadata.get("github_style", "widget")  # "widget" or "icon"

        # Add or update navbar
        if "navbar" not in config["website"]:
            navbar_config = {
                "left": [
                    {"text": "Home", "href": "index.qmd"},
                    {"text": "Reference", "href": "reference/index.qmd"},
                ]
            }

            # Add GitHub link on the right if repository URL is available
            if owner and repo and repo_url and github_style == "widget":
                gh_widget_html = (
                    f'<div id="github-widget" data-owner="{owner}" data-repo="{repo}"></div>'
                )
                navbar_config["right"] = [{"text": gh_widget_html}]
            elif repo_url:
                navbar_config["right"] = [{"icon": "github", "href": repo_url}]

            config["website"]["navbar"] = navbar_config
        else:
            # Update existing navbar: upgrade icon to widget if configured
            self._update_navbar_github_link(config, owner, repo, repo_url, github_style)

        # Add GitHub widget script to page if using widget style
        if owner and repo and github_style == "widget":
            if "include-after-body" not in config["format"]["html"]:
                config["format"]["html"]["include-after-body"] = []
            elif isinstance(config["format"]["html"]["include-after-body"], str):
                config["format"]["html"]["include-after-body"] = [
                    config["format"]["html"]["include-after-body"]
                ]

            # Add the GitHub widget script
            gh_script_entry = {"text": '<script src="github-widget.js"></script>'}
            if gh_script_entry not in config["format"]["html"]["include-after-body"]:
                # Check if github-widget.js is already included
                has_gh_widget = any(
                    "github-widget" in str(item)
                    for item in config["format"]["html"]["include-after-body"]
                )
                if not has_gh_widget:
                    config["format"]["html"]["include-after-body"].append(gh_script_entry)

        # Add sidebar filter script if enabled
        if metadata.get("sidebar_filter_enabled", True):
            if "include-after-body" not in config["format"]["html"]:
                config["format"]["html"]["include-after-body"] = []
            elif isinstance(config["format"]["html"]["include-after-body"], str):
                config["format"]["html"]["include-after-body"] = [
                    config["format"]["html"]["include-after-body"]
                ]

            # Add the sidebar filter script
            filter_script_entry = {"text": '<script src="sidebar-filter.js"></script>'}
            has_filter = any(
                "sidebar-filter" in str(item)
                for item in config["format"]["html"]["include-after-body"]
            )
            if not has_filter:
                config["format"]["html"]["include-after-body"].append(filter_script_entry)

            # Add data attributes for configuration (must be before sidebar-filter.js)
            min_items = metadata.get("sidebar_filter_min_items", 20)
            if min_items != 20:
                # Add custom min_items via body data attribute and insert BEFORE the filter script
                min_items_script = {
                    "text": f'<script>document.body.dataset.sidebarFilterMinItems = "{min_items}";</script>'
                }
                has_min_items = any(
                    "sidebarFilterMinItems" in str(item)
                    for item in config["format"]["html"]["include-after-body"]
                )
                if not has_min_items:
                    # Insert before the sidebar-filter.js script
                    filter_index = next(
                        (
                            i
                            for i, item in enumerate(config["format"]["html"]["include-after-body"])
                            if "sidebar-filter" in str(item)
                        ),
                        len(config["format"]["html"]["include-after-body"]),
                    )
                    config["format"]["html"]["include-after-body"].insert(
                        filter_index, min_items_script
                    )

        # Add dark mode toggle script (if enabled)
        dark_mode_enabled = metadata.get("dark_mode_toggle_enabled", True)
        if dark_mode_enabled:
            if "include-after-body" not in config["format"]["html"]:
                config["format"]["html"]["include-after-body"] = []
            elif isinstance(config["format"]["html"]["include-after-body"], str):
                config["format"]["html"]["include-after-body"] = [
                    config["format"]["html"]["include-after-body"]
                ]

            dark_mode_script_entry = {"text": '<script src="dark-mode-toggle.js"></script>'}
            has_dark_mode = any(
                "dark-mode-toggle" in str(item)
                for item in config["format"]["html"]["include-after-body"]
            )
            if not has_dark_mode:
                config["format"]["html"]["include-after-body"].append(dark_mode_script_entry)

            # Add early theme detection script in header to prevent flash of wrong theme
            if "include-in-header" not in config["format"]["html"]:
                config["format"]["html"]["include-in-header"] = []
            elif isinstance(config["format"]["html"]["include-in-header"], str):
                config["format"]["html"]["include-in-header"] = [
                    config["format"]["html"]["include-in-header"]
                ]

            # Reference external script file for early theme detection (cleaner YAML)
            early_theme_script = {"text": '<script src="theme-init.js"></script>'}
            has_early_theme = any(
                "theme-init" in str(item) for item in config["format"]["html"]["include-in-header"]
            )
            if not has_early_theme:
                config["format"]["html"]["include-in-header"].append(early_theme_script)

        # Add reference switcher script (if CLI is enabled)
        cli_enabled = metadata.get("cli_enabled", False)
        if cli_enabled:
            if "include-after-body" not in config["format"]["html"]:
                config["format"]["html"]["include-after-body"] = []
            elif isinstance(config["format"]["html"]["include-after-body"], str):
                config["format"]["html"]["include-after-body"] = [
                    config["format"]["html"]["include-after-body"]
                ]

            ref_switcher_script_entry = {"text": '<script src="reference-switcher.js"></script>'}
            has_ref_switcher = any(
                "reference-switcher" in str(item)
                for item in config["format"]["html"]["include-after-body"]
            )
            if not has_ref_switcher:
                config["format"]["html"]["include-after-body"].append(ref_switcher_script_entry)

        # Add sidebar navigation for reference pages
        if "sidebar" not in config["website"]:
            config["website"]["sidebar"] = [
                {
                    "id": "reference",
                    "contents": "reference/",
                }
            ]

        # Add page footer with copyright notice if not present
        if "page-footer" not in config["website"]:
            import datetime

            current_year = datetime.datetime.now().year
            metadata = self._get_package_metadata()

            # Get author name from metadata
            author_name = None
            if metadata.get("authors"):
                first_author = metadata["authors"][0]
                if isinstance(first_author, dict):
                    author_name = first_author.get("name")
                elif isinstance(first_author, str):
                    author_name = first_author

            if author_name:
                config["website"]["page-footer"] = {"left": f"&copy; {current_year} {author_name}"}

        # Write back to file
        self._write_quarto_yml(quarto_yml, config)

        print(f"Updated {quarto_yml} with great-docs configuration")

    def _update_sidebar_from_sections(self) -> None:
        """
        Update sidebar navigation based on quartodoc sections.

        Builds a structured sidebar with sections and their contents, and excludes the index page
        from showing the sidebar.
        """
        quarto_yml = self.project_path / "_quarto.yml"

        if not quarto_yml.exists():
            return

        with open(quarto_yml, "r") as f:
            config = yaml.safe_load(f) or {}

        # Get quartodoc sections if they exist
        if "quartodoc" not in config or "sections" not in config["quartodoc"]:
            return

        sections = config["quartodoc"]["sections"]
        sidebar_contents = []

        # Build sidebar structure from sections
        for section in sections:
            section_entry = {"section": section["title"], "contents": []}

            # Add each item in the section
            for item in section.get("contents", []):
                # Handle both string and dict formats
                if isinstance(item, str):
                    section_entry["contents"].append(f"reference/{item}.qmd")
                elif isinstance(item, dict):
                    # Extract the name from dict format (e.g., {'name': 'Graph', 'members': []})
                    item_name = item.get("name", str(item))
                    section_entry["contents"].append(f"reference/{item_name}.qmd")
                else:
                    # Fallback for unexpected types
                    section_entry["contents"].append(f"reference/{item}.qmd")

            sidebar_contents.append(section_entry)

        # Update sidebar configuration
        if "website" not in config:
            config["website"] = {}

        # Build sidebar with API link at top (not subject to filtering)
        # followed by the sectioned contents
        full_contents = [
            {"text": "API", "href": "reference/index.qmd"},
        ] + sidebar_contents

        config["website"]["sidebar"] = [
            {
                "id": "reference",
                "contents": full_contents,
            }
        ]

        # Write back
        self._write_quarto_yml(quarto_yml, config)

    def _update_reference_index_frontmatter(self) -> None:
        """Ensure reference/index.qmd has proper frontmatter."""
        index_path = self.docs_dir / "reference" / "index.qmd"

        if not index_path.exists():
            return

        # Read the current content
        with open(index_path, "r") as f:
            content = f.read()

        # Check if frontmatter already exists; if so, leave it as is
        if content.startswith("---"):
            return

        # Add minimal frontmatter if none exists
        content = f"---\n---\n\n{content}"

        # Write updated content
        with open(index_path, "w") as f:
            f.write(content)

    def _generate_llms_txt(self) -> None:
        """
        Generate an llms.txt file for LLM documentation indexing.

        Creates a structured markdown file that indexes the API reference pages, following the
        llms.txt standard format for LLM-readable documentation. The file is saved to the docs
        directory and will be included in the built site.

        The format follows the structure:

        - package title with description
        - API Reference section with links to each documented item
        """
        quarto_yml = self.project_path / "_quarto.yml"

        if not quarto_yml.exists():
            return

        with open(quarto_yml, "r") as f:
            config = yaml.safe_load(f) or {}

        # Get quartodoc sections and package info
        if "quartodoc" not in config:
            return

        quartodoc_config = config["quartodoc"]
        sections = quartodoc_config.get("sections", [])
        package_name = quartodoc_config.get("package")

        if not package_name or not sections:
            return

        # Get package metadata for description and site URL
        metadata = self._get_package_metadata()
        description = metadata.get("description", "")

        # Get the site URL and prefer the Documentation URL from pyproject.toml,
        # fall back to site-url from _quarto.yml
        urls = metadata.get("urls", {})
        site_url = urls.get("Documentation", "") or config.get("website", {}).get("site-url", "")

        # Clean up site URL and remove any trailing anchors or paths that aren't the base
        if site_url:
            # Remove trailing #readme or similar anchors
            if "#" in site_url:
                site_url = site_url.split("#")[0]
            # Ensure trailing slash
            if not site_url.endswith("/"):
                site_url += "/"

        # Build the llms.txt content
        lines = []

        # Header with package name
        lines.append(f"# {package_name}")
        lines.append("")

        # Description
        if description:
            lines.append(f"> {description}")
            lines.append("")

        # API Reference section
        lines.append("## Docs")
        lines.append("")
        lines.append("### API Reference")
        lines.append("")

        # Process each section
        for section in sections:
            section_title = section.get("title", "")
            section_desc = section.get("desc", "")

            # Add section header as a comment or sub-heading if there are multiple sections
            if len(sections) > 1 and section_title:
                lines.append(f"#### {section_title}")
                if section_desc:
                    lines.append(f"> {section_desc}")
                lines.append("")

            # Add each item in the section
            for item in section.get("contents", []):
                # Handle both string and dict formats
                if isinstance(item, str):
                    item_name = item
                    item_desc = ""
                elif isinstance(item, dict):
                    item_name = item.get("name", str(item))
                    item_desc = ""
                else:
                    continue

                # Get description from docstring if available
                if not item_desc:
                    item_desc = self._get_docstring_summary(package_name, item_name)

                # Build the URL
                if site_url:
                    url = f"{site_url}reference/{item_name}.html"
                else:
                    url = f"reference/{item_name}.html"

                # Format the line
                if item_desc:
                    lines.append(f"- [{item_name}]({url}): {item_desc}")
                else:
                    lines.append(f"- [{item_name}]({url})")

            lines.append("")

        # Write the llms.txt file
        llms_txt_path = self.project_path / "llms.txt"
        with open(llms_txt_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        print(f"Created {llms_txt_path}")

    def _get_docstring_summary(self, package_name: str, item_name: str) -> str:
        """
        Get the first line of a docstring for an item.

        Parameters
        ----------
        package_name
            The name of the package containing the item.
        item_name
            The name of the class, function, or module to get the docstring for.

        Returns
        -------
        str
            The first line of the docstring, or empty string if not available.
        """
        try:
            import importlib

            # Normalize package name
            normalized_name = package_name.replace("-", "_")
            module = importlib.import_module(normalized_name)

            # Try to get the object
            obj = getattr(module, item_name, None)
            if obj is None:
                return ""

            # Get docstring
            docstring = getattr(obj, "__doc__", None)
            if not docstring:
                return ""

            # Extract first line/sentence
            first_line = docstring.strip().split("\n")[0].strip()

            # Clean up the line (remove trailing periods, normalize whitespace)
            first_line = first_line.rstrip(".")

            return first_line

        except Exception:
            return ""

    def _generate_llms_full_txt(self) -> None:
        """
        Generate an llms-full.txt file with comprehensive API documentation for LLMs.

        Creates a detailed markdown file containing full function/class signatures and
        docstrings, plus CLI help text if available. This provides comprehensive context
        for LLM-based tools to understand and work with the package's API.

        The format includes:
        - Package header with description
        - Full API documentation with signatures and docstrings organized by family/section
        - CLI documentation with --help output for all commands
        """

        quarto_yml = self.project_path / "_quarto.yml"

        if not quarto_yml.exists():
            return

        with open(quarto_yml, "r") as f:
            config = yaml.safe_load(f) or {}

        # Get quartodoc sections and package info
        if "quartodoc" not in config:
            return

        quartodoc_config = config["quartodoc"]
        sections = quartodoc_config.get("sections", [])
        package_name = quartodoc_config.get("package")

        if not package_name:
            return

        # Get package metadata
        metadata = self._get_package_metadata()
        description = metadata.get("description", "")

        # Import the package
        try:
            import importlib

            normalized_name = package_name.replace("-", "_")
            module = importlib.import_module(normalized_name)
        except ImportError as e:
            print(f"Could not import {package_name}: {e}")
            return

        sep_line = "-" * 70

        # Build the content
        lines = []
        lines.append(sep_line)
        lines.append(f"This is the API documentation for the {package_name} library.")
        lines.append(sep_line)
        lines.append("")

        # Process each section
        for section in sections:
            section_title = section.get("title", "")
            section_desc = section.get("desc", "")

            # Add section header
            if section_title:
                lines.append(f"\n## {section_title}\n")
                if section_desc:
                    lines.append(f"{section_desc}\n")
                lines.append("")

            # Process each item in the section
            for item in section.get("contents", []):
                # Handle both string and dict formats
                if isinstance(item, str):
                    item_name = item
                elif isinstance(item, dict):
                    item_name = item.get("name", str(item))
                else:
                    continue

                # Get the object's signature and docstring
                api_text = self._get_api_details(module, item_name)
                if api_text:
                    lines.append(api_text)
                    lines.append("")

        # Add CLI documentation if enabled
        cli_text = self._get_cli_help_text_for_llms()
        if cli_text:
            lines.append("")
            lines.append(sep_line)
            lines.append("This is the CLI documentation for the package.")
            lines.append(sep_line)
            lines.append("")
            lines.append(cli_text)

        # Add User Guide content if available
        user_guide_text = self._get_user_guide_text_for_llms()
        if user_guide_text:
            lines.append("")
            lines.append(sep_line)
            lines.append("This is the User Guide documentation for the package.")
            lines.append(sep_line)
            lines.append("")
            lines.append(user_guide_text)

        # Write the llms-full.txt file
        llms_full_path = self.project_path / "llms-full.txt"
        with open(llms_full_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        print(f"Created {llms_full_path}")

    def _get_api_details(self, module, item_name: str) -> str:
        """
        Retrieve the signature and docstring for a function/class.

        Parameters
        ----------
        module
            The module from which to retrieve the function/class.
        item_name
            The name of the function or class (can be dotted for nested attributes).

        Returns
        -------
        str
            A string containing the name, signature, and docstring.
        """
        import inspect

        try:
            # Split the attribute path to handle nested attributes (e.g., "Class.method")
            parts = item_name.split(".")
            obj = module
            for part in parts:
                obj = getattr(obj, part)

            # Get the name of the object
            obj_name = getattr(obj, "__name__", item_name)

            # Get the function/class signature
            try:
                sig = inspect.signature(obj)
                sig_str = f"{obj_name}{sig}"
            except (ValueError, TypeError):
                # Some objects don't have signatures (e.g., some built-ins)
                sig_str = obj_name

            # Get the docstring
            doc = getattr(obj, "__doc__", None) or ""

            # Clean up the docstring - remove excessive indentation
            if doc:
                doc = inspect.cleandoc(doc)

            # Combine the signature and docstring
            return f"{sig_str}\n\n{doc}" if doc else sig_str

        except AttributeError:
            return ""
        except Exception:
            return ""

    def _get_cli_help_text_for_llms(self) -> str:
        """
        Get CLI help text formatted for llms-full.txt.

        Returns
        -------
        str
            Formatted CLI help text with all commands and subcommands.
        """
        metadata = self._get_package_metadata()

        if not metadata.get("cli_enabled", False):
            return ""

        package_name = self._detect_package_name()
        if not package_name:
            return ""

        cli_info = self._discover_click_cli(package_name)
        if not cli_info:
            return ""

        lines = []
        entry_point = cli_info.get("entry_point_name", package_name)

        # Main CLI help
        lines.append(f"## CLI: {entry_point}")
        lines.append("")
        lines.append("```")
        lines.append(cli_info.get("help_text", ""))
        lines.append("```")
        lines.append("")

        # Process subcommands recursively
        def add_subcommand_help(cmd_info: dict, depth: int = 0):
            nonlocal lines
            for subcmd in cmd_info.get("commands", []):
                if subcmd.get("hidden"):
                    continue

                full_path = subcmd.get("full_path", subcmd.get("name", ""))
                lines.append(f"### {full_path}")
                lines.append("")
                lines.append("```")
                lines.append(subcmd.get("help_text", ""))
                lines.append("```")
                lines.append("")

                # Recurse for nested subcommands
                if subcmd.get("commands"):
                    add_subcommand_help(subcmd, depth + 1)

        add_subcommand_help(cli_info)

        return "\n".join(lines)

    def _get_user_guide_text_for_llms(self) -> str:
        """
        Get User Guide content formatted for llms-full.txt.

        Reads all user guide .qmd files in order and extracts their content,
        stripping YAML frontmatter but preserving the document structure.

        Returns
        -------
        str
            Formatted User Guide content with all pages in order.
        """
        user_guide_info = self._discover_user_guide()
        if not user_guide_info:
            return ""

        lines = []
        files = user_guide_info.get("files", [])

        if not files:
            return ""

        current_section = None

        for file_info in files:
            file_path = file_info.get("path")
            title = file_info.get("title", "")
            section = file_info.get("section")

            if not file_path or not file_path.exists():
                continue

            # Add section header if this is a new section
            if section and section != current_section:
                lines.append(f"\n## {section}\n")
                current_section = section

            # Read the file content
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
            except Exception:
                continue

            # Strip YAML frontmatter
            content = self._strip_frontmatter(content)

            # Add the page title as a header if not already starting with one
            content_stripped = content.strip()
            if not content_stripped.startswith("#"):
                lines.append(f"### {title}\n")

            lines.append(content)
            lines.append("")

        return "\n".join(lines)

    def _strip_frontmatter(self, content: str) -> str:
        """
        Remove YAML frontmatter from a document.

        Parameters
        ----------
        content
            The document content.

        Returns
        -------
        str
            The content with frontmatter removed.
        """
        if content.startswith("---"):
            parts = content.split("---", 2)
            if len(parts) >= 3:
                return parts[2].lstrip()
        return content

    def uninstall(self) -> None:
        """
        Remove great-docs configuration and build directory from the project.

        This method deletes the great-docs.yml configuration file and the great-docs/
        build directory (if it exists).

        Examples
        --------
        Uninstall great-docs from the current directory:

        ```python
        from great_docs import GreatDocs

        docs = GreatDocs()
        docs.uninstall()
        ```

        Uninstall from a specific project directory:

        ```python
        docs = GreatDocs("/path/to/my/project")
        docs.uninstall()
        ```
        """
        print("Uninstalling great-docs from your project...")

        # Remove the great-docs.yml configuration file
        config_path = self.project_root / "great-docs.yml"
        if config_path.exists():
            config_path.unlink()
            print(f"Removed {config_path.relative_to(self.project_root)}")

        # Remove the great-docs/ build directory if it exists
        if self.project_path.exists():
            shutil.rmtree(self.project_path)
            print(f"Removed {self.project_path.relative_to(self.project_root)}/ directory")

        print("✅ Great-docs uninstalled successfully!")

    def build(self, watch: bool = False, refresh: bool = True) -> None:
        """
        Build the documentation site.

        Runs `quartodoc build` followed by `quarto render`. By default, re-discovers package exports
        and updates the quartodoc configuration before building.

        Parameters
        ----------
        watch
            If `True`, watch for changes and rebuild automatically.
        refresh
            If `True` (default), re-discover package exports and update quartodoc config before
            building. Set to False for faster rebuilds when your package API hasn't changed.

        Examples
        --------
        Build the documentation (with API refresh):

        ```python
        from great_docs import GreatDocs

        docs = GreatDocs()
        docs.build()
        ```

        Build with watch mode:

        ```python
        docs.build(watch=True)
        ```

        Quick rebuild without API refresh:

        ```python
        docs.build(refresh=False)
        ```
        """
        import subprocess
        import sys
        import threading
        import time

        def show_progress(stop_event, message):
            """Show a simple spinner while command is running."""
            spinner = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧", "⠇", "⠏"]
            idx = 0
            while not stop_event.is_set():
                print(f"\r{message} {spinner[idx % len(spinner)]}", end="", flush=True)
                idx += 1
                time.sleep(0.1)
            print(f"\r{message} ", end="", flush=True)

        print("Building documentation with great-docs...")

        # Prepare the build directory with all assets and configuration
        self._prepare_build_directory()

        # Change to build directory
        original_dir = os.getcwd()
        try:
            os.chdir(self.project_path)

            # Step 0.5: Refresh quartodoc config if requested
            if refresh:
                print("\n🔄 Refreshing quartodoc configuration...")
                self._refresh_quartodoc_config()

            # Step 0.6: Generate llms.txt and llms-full.txt files
            print("\n📝 Generating llms.txt and llms-full.txt...")
            self._generate_llms_txt()
            self._generate_llms_full_txt()

            # Step 0.7: Generate source links JSON
            print("\n🔗 Generating source links...")
            package_name = self._detect_package_name()
            if package_name:
                self._generate_source_links_json(package_name)

            # Step 0.8: Generate CLI documentation if enabled
            metadata = self._get_package_metadata()
            if metadata.get("cli_enabled", False):
                print("\n🖥️  Generating CLI reference...")
                try:
                    cli_info = self._discover_click_cli(package_name)
                    if cli_info:
                        cli_files = self._generate_cli_reference_pages(cli_info)
                        if cli_files:
                            self._update_sidebar_with_cli(cli_files)
                            print(f"✅ Generated {len(cli_files)} CLI reference page(s)")
                    else:
                        print("   No Click CLI found or CLI documentation disabled")
                except Exception as e:
                    print(f"   ⚠️  Error generating CLI documentation: {e}")
                    import traceback

                    traceback.print_exc()

            # Step 0.9: Process User Guide if present
            try:
                self._process_user_guide()
            except Exception as e:
                print(f"   ⚠️  Error processing User Guide: {e}")
                import traceback

                traceback.print_exc()

            # Step 1: Run quartodoc build using Python module execution
            # This ensures it uses the same Python environment as great-docs
            print("\n📚 Step 1: Generating API reference with quartodoc...")

            stop_event = threading.Event()
            progress_thread = threading.Thread(
                target=show_progress, args=(stop_event, "   Processing")
            )
            progress_thread.start()

            # Get environment with PYTHONPATH set to include the package root
            # This ensures griffe/quartodoc can find the package even if not installed
            quartodoc_env = self._get_quarto_env()

            result = subprocess.run(
                [sys.executable, "-m", "quartodoc", "build"],
                capture_output=True,
                text=True,
                env=quartodoc_env,
            )

            stop_event.set()
            progress_thread.join()

            if result.returncode != 0:
                print("\n❌ quartodoc build failed:")
                # Check if quartodoc is not installed
                if "No module named quartodoc" in result.stderr:
                    print("\n⚠️  quartodoc is not installed in your environment.")
                    print("\nTo fix this, install quartodoc:")
                    print(f"  {sys.executable} -m pip install quartodoc")
                    print("\nOr if using pip directly:")
                    print("  pip install quartodoc")
                else:
                    print(result.stderr)
                sys.exit(1)
            else:
                print("\n✅ API reference generated")

            # Get environment with QUARTO_PYTHON set for proper Python detection
            quarto_env = self._get_quarto_env()

            # Step 2: Run quarto render or preview
            if watch:
                print("\n🔄 Step 2: Starting Quarto in watch mode...")
                print("Press Ctrl+C to stop watching")
                subprocess.run(["quarto", "preview", "--no-browser"], env=quarto_env)
            else:
                print("\n🔨 Step 2: Building site with Quarto...")

                stop_event = threading.Event()
                progress_thread = threading.Thread(
                    target=show_progress, args=(stop_event, "   Rendering")
                )
                progress_thread.start()

                result = subprocess.run(
                    ["quarto", "render"], capture_output=True, text=True, env=quarto_env
                )

                stop_event.set()
                progress_thread.join()

                if result.returncode != 0:
                    print("\n❌ quarto render failed:")
                    print(result.stderr)
                    sys.exit(1)
                else:
                    print("\n✅ Site built successfully")
                    site_path = self.project_path / "_site" / "index.html"
                    if site_path.exists():
                        print(f"\n🎉 Your site is ready! Open: {site_path}")
                    else:
                        print(f"\n🎉 Your site is ready in: {self.project_path / '_site'}")

        finally:
            os.chdir(original_dir)

    def preview(self) -> None:
        """
        Preview the documentation site locally.

        Opens the built site in the default browser. If the site hasn't been built yet,
        it will be built first. Use `great-docs build` to rebuild the site if you've
        made changes.

        Examples
        --------
        Preview the documentation:

        ```python
        from great_docs import GreatDocs

        docs = GreatDocs()
        docs.preview()
        ```
        """
        import sys
        import webbrowser

        print("Previewing documentation...")

        # Check if site has been built
        site_path = self.project_path / "_site"
        index_html = site_path / "index.html"

        if not index_html.exists():
            print("Site not found, building first...")
            self.build()

        # Open the site in the default browser
        if index_html.exists():
            print(f"\n🌐 Opening site in browser: {index_html}")
            webbrowser.open(f"file://{index_html.absolute()}")
        else:
            print("❌ Could not find built site")
            sys.exit(1)

    def check_links(
        self,
        include_source: bool = True,
        include_docs: bool = True,
        timeout: float = 10.0,
        ignore_patterns: list[str] | None = None,
        verbose: bool = False,
    ) -> dict:
        """
        Check all links in source code and documentation for broken links.

        This method scans Python source files and documentation files (`.qmd`, `.md`)
        for URLs and checks their HTTP status. It reports broken links (404s) and
        warns about redirects.

        The following content is automatically excluded from link checking:

        - **Python comments**: URLs in lines starting with `#`
        - **Code blocks**: URLs inside fenced code blocks (````` ... `````)
        - **Inline code**: URLs inside backticks (`` `...` ``)
        - **Marked URLs**: URLs followed by `{.gd-no-link}` in `.qmd`/`.md` files

        For documentation, the checker scans the source `user_guide/` directory
        rather than the generated `docs/` directory to avoid checking transient files.

        In `.qmd` files, you can exclude specific URLs from checking by adding
        `{.gd-no-link}` immediately after the URL::

            Visit http://example.com{.gd-no-link} for an example.
            Also works with inline code: `http://example.com`{.gd-no-link}

        Parameters
        ----------
        include_source
            If `True`, scan Python source files in the package directory for URLs.
            Default is `True`.
        include_docs
            If `True`, scan documentation files (`.qmd`, `.md`) for URLs.
            Default is `True`.
        timeout
            Timeout in seconds for each HTTP request. Default is `10.0`.
        ignore_patterns
            List of URL patterns (strings or regex) to ignore. URLs matching any
            pattern will be skipped. Default is `None`.
        verbose
            If `True`, print detailed progress information. Default is `False`.

        Returns
        -------
        dict
            A dictionary containing:
            - `total`: Total number of unique links checked
            - `ok`: List of links that returned 2xx status
            - `redirects`: List of dicts with `url`, `status`, `location` for 3xx responses
            - `broken`: List of dicts with `url`, `status`, `error` for 4xx/5xx or errors
            - `skipped`: List of URLs that were skipped (matched ignore patterns)
            - `by_file`: Dict mapping file paths to lists of links found in each file

        Examples
        --------
        Check all links in a project:

        ```python
        from great_docs import GreatDocs

        docs = GreatDocs()
        results = docs.check_links()

        print(f"Checked {results['total']} links")
        print(f"Broken: {len(results['broken'])}")
        print(f"Redirects: {len(results['redirects'])}")
        ```

        Check only documentation files with custom timeout:

        ```python
        results = docs.check_links(
            include_source=False,
            timeout=5.0,
            ignore_patterns=["localhost", "127.0.0.1", "example.com"]
        )
        ```
        """
        import requests

        # URL regex pattern - matches http and https URLs
        url_pattern = re.compile(
            r'https?://[^\s<>"\')\]}`\\]+',
            re.IGNORECASE,
        )

        # Pattern to detect URLs marked with {.gd-no-link} in .qmd files
        # This allows marking example/fake links for exclusion: http://example.com{.gd-no-link}
        # Also handles URLs in inline code: `http://example.com`{.gd-no-link}
        gd_no_link_pattern = re.compile(
            r'`?(https?://[^\s<>"\')\]}`\\{]+)`?\{\.gd-no-link\}',
            re.IGNORECASE,
        )

        # Compile ignore patterns
        ignore_regexes = []
        if ignore_patterns:
            for pattern in ignore_patterns:
                try:
                    ignore_regexes.append(re.compile(pattern, re.IGNORECASE))
                except re.error:
                    # Treat as literal string if not valid regex
                    ignore_regexes.append(re.compile(re.escape(pattern), re.IGNORECASE))

        # Collect all files to scan
        files_to_scan: list[Path] = []

        if include_source:
            # Find package directory
            package_name = self._detect_package_name()
            if package_name:
                package_dir = self.project_root / package_name.replace("-", "_")
                if package_dir.exists():
                    files_to_scan.extend(package_dir.rglob("*.py"))

        if include_docs:
            # Scan documentation files
            # Priority: if user_guide/ exists, scan that (it's the source)
            # Otherwise, scan the docs directory directly

            user_guide_dir = self.project_root / "user_guide"
            if user_guide_dir.exists():
                # Scan user_guide source directory instead of generated docs/
                files_to_scan.extend(user_guide_dir.rglob("*.qmd"))
                files_to_scan.extend(user_guide_dir.rglob("*.md"))
            elif self.project_path.exists():
                # No user_guide/, scan docs directory directly
                files_to_scan.extend(self.project_path.rglob("*.qmd"))
                files_to_scan.extend(self.project_path.rglob("*.md"))

            # Also check README in project root
            readme = self.project_root / "README.md"
            if readme.exists():
                files_to_scan.append(readme)

        # Extract URLs from all files
        url_to_files: dict[str, list[str]] = {}
        by_file: dict[str, list[str]] = {}

        for file_path in files_to_scan:
            try:
                content = file_path.read_text(encoding="utf-8", errors="ignore")

                # For .qmd and .md files, find URLs marked with {.gd-no-link} and exclude them
                # Also strip code blocks to avoid checking example URLs
                excluded_urls: set[str] = set()
                if file_path.suffix in (".qmd", ".md"):
                    for match in gd_no_link_pattern.finditer(content):
                        excluded_urls.add(match.group(1))

                    # Remove fenced code blocks (``` ... ```) before URL extraction
                    # This prevents example URLs in code blocks from being checked
                    content = re.sub(r"```[^`]*```", "", content, flags=re.DOTALL)

                    # Also remove inline code (`...`) to avoid example URLs
                    content = re.sub(r"`[^`]+`", "", content)

                # For Python files, exclude URLs in comments (lines starting with #)
                # This prevents example URLs in code comments from being checked
                if file_path.suffix == ".py":
                    # Remove single-line comments before URL extraction
                    lines = content.split("\n")
                    non_comment_lines = []
                    for line in lines:
                        # Find the start of a comment (# not inside a string)
                        # Simple approach: strip the comment portion from each line
                        stripped = line.lstrip()
                        if stripped.startswith("#"):
                            # Entire line is a comment, skip it
                            non_comment_lines.append("")
                        else:
                            # Keep the line (inline comments after code are less common
                            # for example URLs, but we keep them for now)
                            non_comment_lines.append(line)
                    content = "\n".join(non_comment_lines)

                urls = url_pattern.findall(content)

                # Clean URLs (remove trailing punctuation)
                cleaned_urls = []
                for url in urls:
                    # Remove trailing punctuation that's likely not part of the URL
                    url = url.rstrip(".,;:!?")
                    # Remove trailing parentheses if unbalanced
                    while url.endswith(")") and url.count(")") > url.count("("):
                        url = url[:-1]
                    # Skip URLs with f-string placeholders (e.g., {variable} or partial {var)
                    # This catches both complete {var} and incomplete {var patterns
                    if "{" in url:
                        continue
                    # Skip URLs marked with {.gd-no-link}
                    if url in excluded_urls:
                        continue
                    cleaned_urls.append(url)

                if cleaned_urls:
                    rel_path = str(file_path.relative_to(self.project_root))
                    by_file[rel_path] = cleaned_urls

                    for url in cleaned_urls:
                        if url not in url_to_files:
                            url_to_files[url] = []
                        url_to_files[url].append(rel_path)

            except Exception as e:
                if verbose:
                    print(f"Warning: Could not read {file_path}: {e}")

        # Check each unique URL
        results = {
            "total": len(url_to_files),
            "ok": [],
            "redirects": [],
            "broken": [],
            "skipped": [],
            "by_file": by_file,
        }

        if verbose:
            print(f"\n🔍 Found {len(url_to_files)} unique URLs to check\n")

        for url in url_to_files:
            # Check if URL matches any ignore pattern
            should_skip = False
            for pattern in ignore_regexes:
                if pattern.search(url):
                    should_skip = True
                    break

            if should_skip:
                results["skipped"].append(url)
                if verbose:
                    print(f"⏭️  Skipped: {url}")
                continue

            try:
                # Use HEAD request first (faster), fall back to GET if needed
                response = requests.head(
                    url,
                    timeout=timeout,
                    allow_redirects=False,
                    headers={"User-Agent": "great-docs-link-checker/1.0"},
                )

                # Some servers don't support HEAD, try GET
                if response.status_code == 405:
                    response = requests.get(
                        url,
                        timeout=timeout,
                        allow_redirects=False,
                        headers={"User-Agent": "great-docs-link-checker/1.0"},
                        stream=True,  # Don't download body
                    )
                    response.close()

                status = response.status_code

                if 200 <= status < 300:
                    results["ok"].append(url)
                    if verbose:
                        print(f"✅ {status} {url}")
                elif 300 <= status < 400:
                    location = response.headers.get("Location", "Unknown")
                    results["redirects"].append(
                        {
                            "url": url,
                            "status": status,
                            "location": location,
                            "files": url_to_files[url],
                        }
                    )
                    if verbose:
                        print(f"↪️  {status} {url} -> {location}")
                else:
                    results["broken"].append(
                        {
                            "url": url,
                            "status": status,
                            "error": f"HTTP {status}",
                            "files": url_to_files[url],
                        }
                    )
                    if verbose:
                        print(f"❌ {status} {url}")

            except requests.exceptions.Timeout:
                results["broken"].append(
                    {
                        "url": url,
                        "status": None,
                        "error": "Timeout",
                        "files": url_to_files[url],
                    }
                )
                if verbose:
                    print(f"⏱️  Timeout: {url}")
            except requests.exceptions.SSLError as e:
                results["broken"].append(
                    {
                        "url": url,
                        "status": None,
                        "error": f"SSL Error: {str(e)[:50]}",
                        "files": url_to_files[url],
                    }
                )
                if verbose:
                    print(f"🔐 SSL Error: {url}")
            except requests.exceptions.ConnectionError:
                results["broken"].append(
                    {
                        "url": url,
                        "status": None,
                        "error": "Connection failed",
                        "files": url_to_files[url],
                    }
                )
                if verbose:
                    print(f"🔌 Connection failed: {url}")
            except Exception as e:
                results["broken"].append(
                    {
                        "url": url,
                        "status": None,
                        "error": str(e)[:100],
                        "files": url_to_files[url],
                    }
                )
                if verbose:
                    print(f"⚠️  Error: {url} - {e}")

        return results

    def spell_check(
        self,
        include_docs: bool = True,
        include_docstrings: bool = False,
        custom_dictionary: list[str] | None = None,
        language: str = "en",
        verbose: bool = False,
    ) -> dict:
        """
        Check spelling in documentation files and optionally docstrings.

        This method scans documentation files (`.qmd`, `.md`) for spelling errors
        using a dictionary-based approach. It intelligently skips code blocks,
        inline code, URLs, and common technical terms.

        Parameters
        ----------
        include_docs
            If `True`, scan documentation files (`.qmd`, `.md`) for spelling errors.
            Default is `True`.
        include_docstrings
            If `True`, also scan Python docstrings in the package.
            Default is `False`.
        custom_dictionary
            List of additional words to consider correct (e.g., project-specific
            terms, library names). Default is `None`.
        language
            Language for spell checking. Currently supports "en" (English).
            Default is `"en"`.
        verbose
            If `True`, print detailed progress information. Default is `False`.

        Returns
        -------
        dict
            A dictionary containing:
            - `total_words`: Total number of words checked
            - `misspelled`: List of dicts with `word`, `suggestions`, `files`, `contexts`
            - `by_file`: Dict mapping file paths to misspelled words in each file
            - `skipped_files`: List of files that couldn't be read

        Examples
        --------
        Check spelling in documentation:

        ```python
        from great_docs import GreatDocs

        docs = GreatDocs()
        results = docs.spell_check()

        print(f"Checked {results['total_words']} words")
        print(f"Misspelled: {len(results['misspelled'])}")

        for item in results['misspelled']:
            print(f"  {item['word']}: {item['suggestions'][:3]}")
        ```

        Check with custom dictionary:

        ```python
        results = docs.spell_check(
            custom_dictionary=["quartodoc", "griffe", "docstring", "navbar"]
        )
        ```
        """
        from spellchecker import SpellChecker

        spell = SpellChecker(language=language)

        # Technical terms commonly found in documentation that should be ignored
        technical_terms = {
            # Programming terms
            "api",
            "apis",
            "cli",
            "sdk",
            "html",
            "css",
            "js",
            "json",
            "yaml",
            "yml",
            "xml",
            "sql",
            "http",
            "https",
            "url",
            "urls",
            "uri",
            "uris",
            "utf",
            "ascii",
            "unicode",
            "regex",
            "regexp",
            "stdin",
            "stdout",
            "stderr",
            "async",
            "await",
            "init",
            "kwargs",
            "args",
            "str",
            "int",
            "bool",
            "dict",
            "tuple",
            "config",
            "configs",
            "env",
            "localhost",
            "dev",
            # Python-specific
            "python",
            "pythonic",
            "pypi",
            "pip",
            "venv",
            "virtualenv",
            "conda",
            "pytest",
            "docstring",
            "docstrings",
            "numpy",
            "numpydoc",
            "pandas",
            "scipy",
            "matplotlib",
            "jupyter",
            "ipython",
            "cython",
            "pypy",
            "pydantic",
            "fastapi",
            "flask",
            "django",
            "setuptools",
            "pyproject",
            # Documentation tools
            "quarto",
            "quartodoc",
            "sphinx",
            "mkdocs",
            "readthedocs",
            "rst",
            "markdown",
            "frontmatter",
            "readme",
            "changelog",
            "roadmap",
            # Great Docs specific
            "griffe",
            "pkgdown",
            "navbar",
            "sidebar",
            "toc",
            "qmd",
            # Git/GitHub
            "git",
            "github",
            "gitlab",
            "bitbucket",
            "repo",
            "repos",
            "pr",
            "prs",
            "ci",
            "cd",
            "workflow",
            "workflows",
            # Common abbreviations
            "e.g",
            "i.e",
            "etc",
            "vs",
            "misc",
            "info",
            "param",
            "params",
            "arg",
            "args",
            "attr",
            "attrs",
            "func",
            "funcs",
            "var",
            "vars",
            "src",
            "dest",
            "dir",
            "dirs",
            "tmp",
            "temp",
            "lib",
            "libs",
            # File extensions/types
            "py",
            "md",
            "txt",
            "csv",
            "pdf",
            "png",
            "jpg",
            "jpeg",
            "svg",
            "gif",
            "ico",
            "webp",
            "toml",
            "cfg",
            "ini",
            "sh",
            "bash",
            "zsh",
            # Miscellaneous tech terms
            "localhost",
            "backend",
            "frontend",
            "fullstack",
            "middleware",
            "webhook",
            "webhooks",
            "oauth",
            "jwt",
            "ssl",
            "tls",
            "tcp",
            "udp",
            "dns",
            "cdn",
            "vm",
            "cpu",
            "gpu",
            "ram",
            "ssd",
            "hdd",
            "os",
            "linux",
            "macos",
            "windows",
            "ubuntu",
            "debian",
            "unix",
            "boolean",
            "booleans",
            "inline",
            "metadata",
            "runtime",
            "subprocess",
            "unicode",
            "utf8",
            "utf-8",
            "serializer",
            "deserialize",
            "lifecycle",
            # Web/cloud services
            "aws",
            "netlify",
            "vercel",
            "cloudflare",
            "heroku",
            "azure",
            # Quarto/documentation specific
            "callout",
            "callouts",
            "backticks",
            "monospace",
            "codeblock",
            "codeblocks",
            "filename",
            "filenames",
            "quickstart",
            "theming",
            "versioned",
            "screenshots",
            # CLI terms
            "subcommand",
            "subcommands",
            "clis",
            "eval",
            "cname",
            # Standards/formats
            "orcid",
            "rfc",
            "llm",
            "llms",
            # Contraction fragments (common in docs)
            "aren",  # aren't
            "doesn",  # doesn't
            "isn",  # isn't
            "shouldn",  # shouldn't
            "wouldn",  # wouldn't
            "couldn",  # couldn't
            "didn",  # didn't
            "hasn",  # hasn't
            "haven",  # haven't
            "wasn",  # wasn't
            "weren",  # weren't
            "won",  # won't (also a real word)
            "ll",  # you'll, we'll
            "ve",  # we've, you've
            # Other tech terms
            "classname",
            "classnames",
            "programmatically",
            "ip",
            "qa",
            "alt",
            "org",
            "greatdocs",
        }

        # Add custom dictionary words
        if custom_dictionary:
            technical_terms.update(word.lower() for word in custom_dictionary)

        # Add technical terms to the spell checker's known words
        spell.word_frequency.load_words(technical_terms)

        # Patterns to skip
        code_block_pattern = re.compile(r"```[\s\S]*?```", re.MULTILINE)
        inline_code_pattern = re.compile(r"`[^`]+`")
        url_pattern = re.compile(r"https?://[^\s<>\"')\]}`\\]+", re.IGNORECASE)
        html_tag_pattern = re.compile(r"<[^>]+>")
        yaml_frontmatter_pattern = re.compile(r"^---\n[\s\S]*?\n---\n", re.MULTILINE)
        directive_pattern = re.compile(r"\{[^}]+\}")  # Quarto directives like {.callout}
        reference_pattern = re.compile(r"@\w+")  # References like @fig-example
        latex_pattern = re.compile(r"\$[^$]+\$")  # LaTeX math
        email_pattern = re.compile(r"[\w.+-]+@[\w.-]+\.\w+")
        # File paths: Unix and relative paths like /path/to/file.py, ./dir/file.qmd
        file_path_pattern = re.compile(r"(?:\.?\.?/[\w./\-_]+)")

        # Collect files to scan
        # Note: We scan source directories (user_guide/) not generated output (docs/)
        # because docs/ is transient and users shouldn't edit files there directly
        files_to_scan: list[Path] = []

        if include_docs:
            # Scan user_guide directory if it exists (source documentation)
            user_guide_dir = self.project_root / "user_guide"
            if user_guide_dir.exists():
                files_to_scan.extend(user_guide_dir.rglob("*.qmd"))
                files_to_scan.extend(user_guide_dir.rglob("*.md"))

            # Also check README in project root
            readme = self.project_root / "README.md"
            if readme.exists():
                files_to_scan.append(readme)

        # Track results
        word_to_files: dict[str, set[str]] = {}
        word_contexts: dict[str, list[str]] = {}
        by_file: dict[str, list[str]] = {}
        skipped_files: list[str] = []
        total_words = 0

        for file_path in files_to_scan:
            try:
                content = file_path.read_text(encoding="utf-8", errors="ignore")
                original_content = content

                # Remove patterns we want to skip
                content = yaml_frontmatter_pattern.sub(" ", content)
                content = code_block_pattern.sub(" ", content)
                content = inline_code_pattern.sub(" ", content)
                content = url_pattern.sub(" ", content)
                content = html_tag_pattern.sub(" ", content)
                content = directive_pattern.sub(" ", content)
                content = reference_pattern.sub(" ", content)
                content = latex_pattern.sub(" ", content)
                content = email_pattern.sub(" ", content)
                content = file_path_pattern.sub(" ", content)

                # Extract words (letters only, min 2 chars)
                words = re.findall(r"\b[a-zA-Z]{2,}\b", content)
                total_words += len(words)

                # Find misspelled words
                misspelled = spell.unknown(words)

                if misspelled:
                    rel_path = str(file_path.relative_to(self.project_root))
                    by_file[rel_path] = list(misspelled)

                    for word in misspelled:
                        word_lower = word.lower()
                        if word_lower not in word_to_files:
                            word_to_files[word_lower] = set()
                            word_contexts[word_lower] = []

                        word_to_files[word_lower].add(rel_path)

                        # Extract context (line containing the word)
                        if len(word_contexts[word_lower]) < 3:  # Limit contexts
                            for line in original_content.split("\n"):
                                if re.search(rf"\b{re.escape(word)}\b", line, re.IGNORECASE):
                                    context = line.strip()[:100]
                                    if context not in word_contexts[word_lower]:
                                        word_contexts[word_lower].append(context)
                                        break

                if verbose:
                    rel_path = str(file_path.relative_to(self.project_root))
                    if misspelled:
                        print(f"📄 {rel_path}: {len(misspelled)} issue(s)")
                    else:
                        print(f"✅ {rel_path}: OK")

            except Exception as e:
                rel_path = str(file_path.relative_to(self.project_root))
                skipped_files.append(rel_path)
                if verbose:
                    print(f"⚠️  Could not read {rel_path}: {e}")

        # Scan docstrings if requested
        if include_docstrings:
            package_name = self._detect_package_name()
            if package_name:
                docstring_misspellings = self._check_docstring_spelling(
                    package_name, spell, verbose
                )
                for word, info in docstring_misspellings.items():
                    word_lower = word.lower()
                    if word_lower not in word_to_files:
                        word_to_files[word_lower] = set()
                        word_contexts[word_lower] = []
                    word_to_files[word_lower].update(info["files"])
                    word_contexts[word_lower].extend(info["contexts"])
                    total_words += info.get("word_count", 0)

        # Build results
        misspelled_list = []
        for word, files in sorted(word_to_files.items()):
            suggestions = list(spell.candidates(word) or [])[:5]
            misspelled_list.append(
                {
                    "word": word,
                    "suggestions": suggestions,
                    "files": sorted(files),
                    "contexts": word_contexts.get(word, []),
                }
            )

        return {
            "total_words": total_words,
            "misspelled": misspelled_list,
            "by_file": by_file,
            "skipped_files": skipped_files,
        }

    def _check_docstring_spelling(
        self,
        package_name: str,
        spell,
        verbose: bool = False,
    ) -> dict:
        """
        Check spelling in package docstrings using griffe.

        Parameters
        ----------
        package_name
            The name of the package to scan.
        spell
            SpellChecker instance to use.
        verbose
            If True, print progress.

        Returns
        -------
        dict
            Mapping of misspelled words to file/context info.
        """
        try:
            import griffe
        except ImportError:
            return {}

        results: dict[str, dict] = {}
        normalized_name = package_name.replace("-", "_")

        try:
            pkg = griffe.load(normalized_name)
        except Exception:
            return {}

        def check_docstring(obj, obj_name: str):
            if not hasattr(obj, "docstring") or not obj.docstring:
                return

            docstring = (
                obj.docstring.value if hasattr(obj.docstring, "value") else str(obj.docstring)
            )

            # Remove code blocks and inline code
            docstring = re.sub(r"```[\s\S]*?```", " ", docstring)
            docstring = re.sub(r"`[^`]+`", " ", docstring)
            docstring = re.sub(r"https?://[^\s]+", " ", docstring)

            words = re.findall(r"\b[a-zA-Z]{2,}\b", docstring)
            misspelled = spell.unknown(words)

            for word in misspelled:
                word_lower = word.lower()
                if word_lower not in results:
                    results[word_lower] = {"files": set(), "contexts": [], "word_count": 0}
                results[word_lower]["files"].add(f"docstring:{obj_name}")
                results[word_lower]["word_count"] += len(words)

                # Add context
                if len(results[word_lower]["contexts"]) < 2:
                    for line in docstring.split("\n"):
                        if word.lower() in line.lower():
                            context = line.strip()[:80]
                            if context:
                                results[word_lower]["contexts"].append(context)
                                break

        def process_members(obj, prefix=""):
            for name, member in obj.members.items():
                if name.startswith("_"):
                    continue

                full_name = f"{prefix}.{name}" if prefix else name

                try:
                    check_docstring(member, full_name)

                    # Process class methods
                    if hasattr(member, "members"):
                        for method_name, method in member.members.items():
                            if not method_name.startswith("_"):
                                check_docstring(method, f"{full_name}.{method_name}")
                except Exception:
                    continue

        try:
            process_members(pkg)
        except Exception:
            pass

        return results
